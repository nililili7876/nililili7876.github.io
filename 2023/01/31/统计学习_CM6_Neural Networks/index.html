<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>统计学习6_NN - Bienvenue en Planète 7876</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#3273dc"><meta name="application-name" content="Icaurs - Hexo Theme"><meta name="msapplication-TileImage" content="icons/touch-icon-iphone.png"><meta name="msapplication-TileColor" content="#3273dc"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Icaurs - Hexo Theme"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="144x144" href="icons/touch-icon-iphone.png"><link rel="apple-touch-icon" sizes="152x152" href="icons/touch-icon-ipad.png"><link rel="apple-touch-icon" sizes="72x72" href="icon/logo.ico"><link rel="apple-touch-icon" sizes="96x96" href="icon/logo.ico"><link rel="apple-touch-icon" sizes="128x128" href="icon/logo.ico"><link rel="apple-touch-icon" sizes="256x256" href="icon/logo.ico"><meta name="description" content="CM6_Neural Networks1Feed-forward neural networks (FFNN) CNN"><meta property="og:type" content="blog"><meta property="og:title" content="111"><meta property="og:url" content="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20221029/ya%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20220917210647.jpg"><meta property="og:site_name" content="Bienvenue en Planète 7876"><meta property="og:description" content="CM6_Neural Networks1Feed-forward neural networks (FFNN) CNN"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230105/yaimage.457bqywy4ys0.png"><meta property="og:image" content="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230105/yaimage.3q0njv0uodu0.png"><meta property="og:image" content="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230105/yaimage.17h04cjkt9og.png"><meta property="og:image" content="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230105/yaimage.69gp6un0zek0.png"><meta property="og:image" content="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230113/yaimage.1vkrljxw6tr4.png"><meta property="og:image" content="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230113/yaimage.5y5cqzbqka00.png"><meta property="og:image" content="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230113/yaimage.1d6hx2i697z4.png"><meta property="og:image" content="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230113/yaimage.4ri79m3voak0.png"><meta property="og:image" content="c:/Users/nili990221/AppData/Roaming/Typora/typora-user-images/image-20230113214430015.png"><meta property="og:image" content="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230113/yaimage.6av42uhjdxk0.png"><meta property="og:image" content="c:/Users/nili990221/AppData/Roaming/Typora/typora-user-images/image-20230113221239827.png"><meta property="og:image" content="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230113/yaimage.299rxkeermzo.png"><meta property="og:image" content="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230113/yaimage.3qw2b944slk0.png"><meta property="og:image" content="c:/Users/nili990221/AppData/Roaming/Typora/typora-user-images/image-20230113233522114.png"><meta property="article:published_time" content="2023-01-30T20:27:19.890Z"><meta property="article:modified_time" content="2023-11-06T09:41:35.102Z"><meta property="article:author" content="7876"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230105/yaimage.457bqywy4ys0.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://cdn.statically.io/gh/nililili7876/blog_pic@main/images/111.webp"},"headline":"统计学习6_NN","image":["https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230105/yaimage.457bqywy4ys0.png","https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230105/yaimage.3q0njv0uodu0.png","https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230105/yaimage.17h04cjkt9og.png","https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230105/yaimage.69gp6un0zek0.png","https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230113/yaimage.1vkrljxw6tr4.png","https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230113/yaimage.5y5cqzbqka00.png","https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230113/yaimage.1d6hx2i697z4.png","https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230113/yaimage.4ri79m3voak0.png","c:/Users/nili990221/AppData/Roaming/Typora/typora-user-images/image-20230113214430015.png","https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230113/yaimage.6av42uhjdxk0.png","c:/Users/nili990221/AppData/Roaming/Typora/typora-user-images/image-20230113221239827.png","https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230113/yaimage.299rxkeermzo.png","https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230113/yaimage.3qw2b944slk0.png","c:/Users/nili990221/AppData/Roaming/Typora/typora-user-images/image-20230113233522114.png"],"datePublished":"2023-01-30T20:27:19.890Z","dateModified":"2023-11-06T09:41:35.102Z","author":{"@type":"Person","name":7876},"publisher":{"@type":"Organization","name":"Bienvenue en Planète 7876","logo":{"@type":"ImageObject","url":"https://cdn.statically.io/gh/nililili7876/blog_pic@main/20221029/ya39f1ada59d3fa9d65440760f6b18c21.jpg"}},"description":"CM6_Neural Networks1Feed-forward neural networks (FFNN) CNN"}</script><link rel="canonical" href="http://nililili7876.cn/2023/01/31/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0_CM6_Neural%20Networks/"><link rel="icon" href="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20221029/ya39f1ada59d3fa9d65440760f6b18c21.jpg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20221029/ya39f1ada59d3fa9d65440760f6b18c21.jpg" alt="Bienvenue en Planète 7876" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com">GitHub</a><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-01-30T20:27:19.890Z" title="2023/1/31 04:27:19">2023-01-30</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-11-06T09:41:35.102Z" title="2023/11/6 17:41:35">2023-11-06</time></span><span class="level-item"><a class="link-muted" href="/categories/NANTES/">NANTES</a><span> / </span><a class="link-muted" href="/categories/NANTES/EI2/">EI2</a><span> / </span><a class="link-muted" href="/categories/NANTES/EI2/EI2-2/">EI2_2</a><span> / </span><a class="link-muted" href="/categories/NANTES/EI2/EI2-2/Stasc/">Stasc</a></span><span class="level-item">23 minutes read (About 3417 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">统计学习6_NN</h1><div class="content"><h1 id="CM6-Neural-Networks1"><a href="#CM6-Neural-Networks1" class="headerlink" title="CM6_Neural Networks1"></a>CM6_Neural Networks1</h1><p>Feed-forward neural networks (FFNN)</p>
<p>CNN</p>
<span id="more"></span>
<h1 id="0-Intro"><a href="#0-Intro" class="headerlink" title="0. Intro"></a>0. Intro</h1><p>Deep learning 的一些领域：Speech Recognition，Computer Vision，NLP</p>
<p>Deep Learning: Representation Learning and non linear transformation</p>
<ul>
<li>Representation learning instead of feature engineering</li>
<li>Deep learning is a set of learning methods attempting to model data with complex architectures combining different transformations.</li>
</ul>
<p>Deep Learning &amp; NN: 神经网络是深度学习的基础，他们连起来形成了深度神经网络DNN。一些发展历史： perceptron —&gt; Thinking Machine —&gt; Deep Learning Networks: old Neural Networks. Their <em>length (number of layers)</em> and <em>width (dimension of the hidden units)</em> have increased over the years.</p>
<p>Three popular examples of NN</p>
<ul>
<li>Feed-forward neural networks (FFNN)</li>
<li>Convolutional neural networks (CNN)</li>
<li>Recurrent neural networks (RNN)</li>
</ul>
<h1 id="1-Neural-Network"><a href="#1-Neural-Network" class="headerlink" title="1. Neural Network"></a>1. Neural Network</h1><p>Artificial Neuron</p>
<p>是一个函数$y=f(x)=g(<w,x>+b)$</p>
<ul>
<li>x: input，==是多个由多个特征组成的向量组成矩阵吗？==</li>
<li>w: 权重组成的向量</li>
<li>b: 偏置项</li>
<li>g: activation function，激活函数，是非线性的</li>
</ul>
<p>示意图($\sum=<w,x>+b$)：</p>
<p><img src="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230105/yaimage.457bqywy4ys0.png" alt="yaimage"></p>
<p><strong>Activation functions</strong></p>
<p>3个流行的激活函数：</p>
<ul>
<li>Sigmoid function(logistic σ): $g(x)=\frac1{1+exp(-x)}$<ul>
<li>可微Differentiable并且数值保持在[0,1]</li>
<li>最常见的激活函数</li>
<li>当|x|趋近于0时它的梯度也趋近于0[causes troubles for the backpropagation algorithm]</li>
</ul>
</li>
<li>Hyperbolic tangent function(tanh) 双曲正接函数: $g(x)=\frac{exp(2x)-1}{exp(2x)+1}$</li>
<li>Rectified Linear Unit (ReLU) activation function: $g(x)=max(0,x)$<ul>
<li>Non linear</li>
<li>在0上不可微 ==&gt; 会有什么影响吗</li>
<li>Sparsification effect[稀疏化效应]: ReLU and ReLU’ are equal to 0 for negative values</li>
<li>Easier to optimize 因为接近线性</li>
</ul>
</li>
</ul>
<h1 id="2-Feedforward-neural-network"><a href="#2-Feedforward-neural-network" class="headerlink" title="2. Feedforward neural network"></a>2. Feedforward neural network</h1><p><img src="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230105/yaimage.3q0njv0uodu0.png" alt="yaimage"></p>
<p>是一种不循环的神经网络，是第一个也是最简单的神经网络</p>
<p>实例：多层感知机/multilayer perceptron</p>
<ul>
<li>多层感知器是由几个隐藏的神经元层组成的结构，其中一层神经元的输出成为下一层神经元的输入。</li>
<li>Layers correspond to functions that are composed in some order:<br>f(x) = f3(f2(f1(x))) has 3 layers.</li>
<li>每一层有$2^j$个神经元==[j????]==和一个激活函数</li>
<li>一般来说神经元数量每层递减</li>
</ul>
<h2 id="2-1-One-hidden-layer-network"><a href="#2-1-One-hidden-layer-network" class="headerlink" title="2.1 One-hidden layer network"></a>2.1 One-hidden layer network</h2><p>我们来看一个单隐藏层的网络</p>
<p><img src="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230105/yaimage.17h04cjkt9og.png" alt="yaimage"></p>
<ul>
<li><p>Pre-activations</p>
<p>进入隐藏层前，要准备：H个权重$w_i$，H个偏置项$b_i$给每个神经元($\sum$)，然后$z(x)=Wx+b$. W是个矩阵(H*d)==&gt;加权线性组合</p>
</li>
<li><p>Activation</p>
<p>h(x) = g(z(x)),g是之前提到的激活函数</p>
<ul>
<li>For regression：输出的层数的width是1</li>
<li>For binary classification：输出的层数的width是1，Sigmoid激活函数给出预测P(Y=1|X=x)</li>
<li>For multi-class classification: The output layer contains one neuron per class k giving a prediction of P(Y = k|X = x),the multidimensional function Softmax is generally used</li>
</ul>
</li>
<li><p>Softmax</p>
<ul>
<li><p>$Softmax(z)_k=\frac1{\sum_{j=1}^kexp(Z_j)}[exp(Z_1)…exp(Z_k)]^T$</p>
</li>
<li><p>不是element-wise activation，把z map到[0,1]然后和为1</p>
</li>
<li>The inputs of the softmax is called the “logits” in deep learning.</li>
</ul>
</li>
</ul>
<p>总结：Mathematical formulation of a one-hidden layer (width H) with softmax output</p>
<p><img src="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230105/yaimage.69gp6un0zek0.png" alt="yaimage"></p>
<p>$y(x)=Softmax(z^{(O)})=Softmax(W^{(O)}h+b^{O})=Softmax(W^{(O)}g(z^{(H)})+B^{(o)}=Softmax(W^{(O)}g(W^{(H)}x+b^{(H)})+B^{(O)})$</p>
<h2 id="2-2-Multilayer-perceptron"><a href="#2-2-Multilayer-perceptron" class="headerlink" title="2.2 Multilayer perceptron"></a>2.2 Multilayer perceptron</h2><p>对于隐藏层l来说</p>
<ul>
<li><p>input</p>
<p>该隐藏层的input是上一个隐藏层的输出，记作$h^{(l-1)}(x)$,然后最开始的$h^{(0)}(x)=x$</p>
</li>
<li><p>Pre-activations</p>
<p>$z^{(l)} (x) = b^{(l)} + W^{(l)}h^{(l−1)} (x)$, $W^{(I)}$的dimension: (width of layer l − 1) ×(width of layer l)==&gt;$w_{ij}^l$表示l层第i个单元和第l-1层第j个单元之间的连接权重</p>
</li>
<li><p>Activations</p>
<p>$h^{(I)}(x) = g^{(I)}(z(x))=g^{(I)}(b^{(l)} + W^{(l)}h^{(l−1)} (x))$</p>
</li>
</ul>
<p>Layers with Keras</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line">model = Sequential( )</span><br><span class="line">model.add (Dense(<span class="number">32</span> , input_dim = <span class="number">784</span>))<span class="comment">#Dense:全连接层[理解成隐藏层的单层？]</span></span><br><span class="line">model.add (Activation(’relu’))</span><br><span class="line">model.add (Dense(<span class="number">10</span> , input_dim = <span class="number">32</span>))</span><br><span class="line">model.add (Activation(’softmax’))</span><br></pre></td></tr></table></figure>
<h2 id="2-3-Training-Feed-Forward-NN"><a href="#2-3-Training-Feed-Forward-NN" class="headerlink" title="2.3. Training Feed Forward NN"></a>2.3. Training Feed Forward NN</h2><p>Main tools to train FFNN and Deep NN: Forward-propagation[前向传播] to compute the loss and to make predictions, Back-propagation to compute gradients,Stochastic optimization algorithms[随机优化].</p>
<p>主要的库: Pytroch , tensorflow,keras…</p>
<p>GPU计算</p>
<h3 id="2-3-1-Loss"><a href="#2-3-1-Loss" class="headerlink" title="2.3.1 Loss"></a>2.3.1 Loss</h3><p>We define loss, risk and empirical risk for each framework</p>
<ol>
<li><p>Classification</p>
<p>Binary Classification为例，希望使得条件概率$P_{\theta}(Y=1|X=x)$尽量大</p>
<p>p和q分布的交叉熵$l(p,q)=-\sum_{x\in X}p(x)log q(x),q=(q_0,q_1)=(1-f(X,\theta),f(X,\theta))$ </p>
<p>然后等于：<img src="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230113/yaimage.1vkrljxw6tr4.png" alt="yaimage"></p>
<ul>
<li>the loss is close to zero when y = 1 with q1 close to one</li>
<li><p>the loss is large when y = 1 with q1 close to zero</p>
<p>cross entropy risk：$R(\theta):=E_{X,Y}[l(Y,f(X,\theta))]$</p>
</li>
</ul>
<p>Empirical risk minimization with cross entropy:<img src="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230113/yaimage.5y5cqzbqka00.png" alt="yaimage"></p>
<p>==&gt;然后得出结论说这样就是在做极大似然估计：is maximizing the log-likelihood of the conditional distribution (Y|X)</p>
<p>Multi-class Classification类似的看不太懂</p>
<p><img src="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230113/yaimage.1d6hx2i697z4.png" alt="yaimage"></p>
<p><img src="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230113/yaimage.4ri79m3voak0.png" alt="yaimage"></p>
</li>
<li><p>Regression: Quadratic loss, associated risk and empirical risk</p>
</li>
<li>Penalized empirical risk: $L^2$和$L^1$/parcimonious</li>
</ol>
<h3 id="2-3-2-Forward-propagation-amp-Backward-propagation"><a href="#2-3-2-Forward-propagation-amp-Backward-propagation" class="headerlink" title="2.3.2 Forward propagation &amp; Backward propagation"></a>2.3.2 Forward propagation &amp; Backward propagation</h3><p>有了对上面损失函数，我们就知道了我们学习的目的是: minimizing $F_n(\theta)=\frac1n\sum_{i=1}^nl(Y_i,f(X_i,\theta))+\lambda\Omega(\theta)$,需要两种方法</p>
<ul>
<li>Forward propagation：计算预测值$f(X,\theta)$,每一层都做</li>
<li>Backward propagation: 计算ER函数的梯度。从输出层开始，一层一层地向前传递，直到输入层，计算误差并对权值和偏置进行更新</li>
<li>FFNN重复进行前向传播和后向传播来确定最终权值的过程叫做训练。在训练过程中，会使用一个数据集，将数据集中的每个样本都用于前向传播和后向传播，这样的一次训练被称为”epoch”。</li>
</ul>
<h4 id="2-3-2-1-Forward-Propagation"><a href="#2-3-2-1-Forward-Propagation" class="headerlink" title="2.3.2.1 Forward Propagation"></a>2.3.2.1 Forward Propagation</h4><hr>
<p>for l = 1 to L do </p>
<p>​    Compute $Z^{(l)} = b^{(l)} + W^{(l)}H^{(l−1)} $#线性加权</p>
<p>​    Compute $H^{(l)} = g^{(l)}Z^{(l)}$#全连接</p>
<p>end for </p>
<p>Compute $Z^{(L+1)} = b^{(L+1)} + W^{(L+1)}H^{(L)}$</p>
<p>Compute $Y’ = Softmax(H^{(L+1)})$ (for classification) </p>
<p>Compute $F_n(θ) = \frac1n\sum^n_{i=1} l(Yi , Yi’) + λΩ(θ) $</p>
<p> return $F_n(θ), Z^{(l)} , H^{(l)} , l = 1 . . . L.$</p>
<hr>
<h4 id="2-3-2-2-Back-propagation"><a href="#2-3-2-2-Back-propagation" class="headerlink" title="2.3.2.2  Back-propagation"></a>2.3.2.2  Back-propagation</h4><p>求梯度+链式法则</p>
<p>Back propagation for softmax multilayer perceptron</p>
<hr>
<p><img src="C:\Users\nili990221\AppData\Roaming\Typora\typora-user-images\image-20230113214430015.png" alt="image-20230113214430015"></p>
<hr>
<h4 id="2-3-2-3-SGD"><a href="#2-3-2-3-SGD" class="headerlink" title="2.3.2.3 SGD"></a>2.3.2.3 <strong>SGD</strong></h4><p>Stochastic gradient descent algorithm: 随机梯度下降, 对大的dataset来说不可能计算每个的梯度，所以在每次迭代, the gradient is computed only on a random batch of size m</p>
<hr>
<p>Initialization : choose of θ (all the weights an bias terms)</p>
<p>for ep = 1 to ept do</p>
<p>​    for t = 1 to n/mt do</p>
<p>​        Pick a subsample B of size m with no replacement</p>
<p>​        计算<img src="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230113/yaimage.6av42uhjdxk0.png" alt="yaimage"></p>
<p>​    end for</p>
<p>end for</p>
<p>return θ</p>
<hr>
<p>学习率$\varepsilon$</p>
<ul>
<li>如果它太小：收敛速度非常慢（可能被困于局部最小值）。</li>
<li>如果太大：围绕着一个最佳状态振荡，而不稳定和收敛。</li>
<li>在训练过程中调整学习率：开始时取大，然后在连续的迭代中减少其值。</li>
</ul>
<p>epochs: B</p>
<ul>
<li>每次迭代时，随机选择m个训练实例来更新参数</li>
<li>迭代的最大次数是以epochs为单位给出的</li>
<li>m是要校准的算法的参数</li>
</ul>
<p>Extensions and variants of SGD: Momentum, AdaGrad, RMSProp, Adam.</p>
<h3 id="2-3-3-Gradient-instability"><a href="#2-3-3-Gradient-instability" class="headerlink" title="2.3.3 Gradient instability"></a>2.3.3 Gradient instability</h3><p>问题：In many situations, gradients often get smaller and smaller as the algorithm progresses down to the lower layers : vanishing gradients problem. In other cases (Recurrent NN) : the gradients can grow bigger and bigger which makes the algorithm diverges: exploding gradients problem. Deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds.</p>
<h4 id="2-3-3-1-RELU-and-its-variants"><a href="#2-3-3-1-RELU-and-its-variants" class="headerlink" title="2.3.3.1 RELU and its variants"></a>2.3.3.1 RELU and its variants</h4><p>ReLU (Rectified Linear Unit) ,激活函数,f(x) = max(0, x),梯度就是0/1，输入负值的神经元会死亡 ==&gt; LeakyReLU: $max(\alpha z,z)$, ELU: x&lt;0时，$\alpha *(exp(x)-1)$</p>
<h4 id="2-3-3-2-Glorot-Xavier-Initialization"><a href="#2-3-3-2-Glorot-Xavier-Initialization" class="headerlink" title="2.3.3.2 Glorot (Xavier) Initialization"></a>2.3.3.2 Glorot (Xavier) Initialization</h4><p>The biases can be initialized to 0. </p>
<p> Initialize the weights at random:不能被初始化为0， $W_{i,j}^k$ are i.i.d. Uniform on [−c, c] with possibly $c=\frac{\sqrt6}{N_k+N_{k-1}},N_k表示隐藏层k的size$</p>
<p>This initialization tends to control the variance of activations.</p>
<h4 id="2-3-3-3-others"><a href="#2-3-3-3-others" class="headerlink" title="2.3.3.3 others"></a>2.3.3.3 others</h4><p><strong>Batch Normalization:</strong> 通过使用批量样本估计均值和方差，对层的输入进行中心化和归一化.</p>
<p><strong>Gradient clipping :</strong> 当梯度变得过大，错误梯度累积，导致网络不稳定时，就会发生梯度爆炸。在反向传播过程中剪切梯度，使其永远不超过某个阈值（这对循环神经网络非常有用）。</p>
<h3 id="2-3-4-Controlling-the-complexity-of-NN"><a href="#2-3-4-Controlling-the-complexity-of-NN" class="headerlink" title="2.3.4. Controlling the complexity of NN"></a>2.3.4. Controlling the complexity of NN</h3><h4 id="2-3-4-1-Number-of-Hidden-Layers"><a href="#2-3-4-1-Number-of-Hidden-Layers" class="headerlink" title="2.3.4.1 Number of Hidden Layers"></a>2.3.4.1 Number of Hidden Layers</h4><p>对于许多问题，一个或两个隐藏层就足以达到高精确度。<br>对于更复杂的问题：逐渐增加隐藏层的数量，直到你开始过度拟合训练集。<br>非常复杂的任务，如大型图像分类或语音识别，通常需要有几十层的网络，还需要大量的训练数据。对于这样复杂的问题，<strong>我们很少需要从头开始训练这样的网络：重新使用预先训练好的执行类似任务的最先进网络的一部分</strong>。训练会快很多，需要的数据也少很多。如果没有预训练的模型可用：在文献中找到一个有效的架构</p>
<h4 id="2-3-4-2-Number-of-Neurons"><a href="#2-3-4-2-Number-of-Neurons" class="headerlink" title="2.3.4.2 Number of Neurons"></a>2.3.4.2 Number of Neurons</h4><p>输入中的神经元数量由输入大小向量决定, 输出中的神经元数量由学习任务决定。</p>
<p>隐蔽层中的神经元数量：size them to form a <em>tunnel</em>, with fewer and fewer neurons at each layer : “low-level features can coalesce into far fewer high-level features”.</p>
<h4 id="2-3-4-3-Regularization"><a href="#2-3-4-3-Regularization" class="headerlink" title="2.3.4.3 Regularization"></a>2.3.4.3 Regularization</h4><ul>
<li><p>early stop</p>
<p>当用梯度下降法拟合参数时，直到一定数量的迭代，新的迭代会改善模型，然而，在这之后，模型的泛化能力会减弱，因为它开始过度拟合训练数据。解决方法：</p>
</li>
</ul>
<p>​            ◮ Splitting : (Train - Validation) - Test. </p>
<p>​            ◮ If validation has not improved for some time, stop and return θ.</p>
<p>For least-squares regression and gradient descent, early stopping can be seen to be equivalent to <em>ridge penalization on the weights</em>.</p>
<ul>
<li><p>dropout</p>
<p>训练一个由基础网络中的非输出单元组成的子网络集合, 在梯度下降的每个迭代中，使用概率p[超参数，对于隐藏层中的单元，一般选择是0.5，对于进入层是0.2]，并且独立于其他单元，网络的每个单元被设置为0。<img src="C:\Users\nili990221\AppData\Roaming\Typora\typora-user-images\image-20230113221239827.png" alt="image-20230113221239827"></p>
</li>
</ul>
<h1 id="3-CNN"><a href="#3-CNN" class="headerlink" title="3.CNN"></a>3.CNN</h1><p>一种专用于处理图像和时间序列的数据，至少在一层用卷积convolution来代替一般的矩阵乘法。CNN里面最重要的两个点：卷积[convolution]和池化[Pooling].</p>
<h2 id="3-1-Filters"><a href="#3-1-Filters" class="headerlink" title="3.1 Filters"></a>3.1 Filters</h2><p>滤波/卷积层的目标是: identify low level features such as edges and curves, and then building up to more abstract concepts through a series of convolutional layers.</p>
<p>卷积层图示：</p>
<p><img src="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230113/yaimage.299rxkeermzo.png" alt="yaimage"></p>
<p>单通道的卷积函数：$Z(i,j)=(I*K)(i,j)=\sum_{|u|&lt;n_1}\sum_{|u|&lt;n_2}I(i+u,j+v)K(u,v)$</p>
<ul>
<li>Z: feature map特征图</li>
<li>I: input</li>
<li>K: kernel/filter, 卷积核的size是$(n_1,n_2)$, 它的值对应于神经元的权值$K(u,v)=w_{u,v}$，由CNN训练得到.</li>
</ul>
<p>过滤器可以看作是特性标识符或特性映射// feature identifiers or feature maps</p>
<p>Parameter sharing: 我们只学习一组参数，而不是为每个位置学习一组单独的权重, 这意味着我们为每个位置学习相同的kernel(或相同的内核集)[一个矩阵划划划]</p>
<h2 id="3-2-Convolution"><a href="#3-2-Convolution" class="headerlink" title="3.2 Convolution"></a>3.2 Convolution</h2><p>Convolutional layers: not one but k filters (take $k = 2^p$ ) that create k feature maps.</p>
<p>multichannel convolution: 在卷积运算中使用多个输入通道，每个通道都有自己的卷积核. Neuron’s receptive field extends across all the previous layers’s<br>feature maps,多通道的卷积函数：$Z(i,j,l)=(V*K)(l,i,j)=\sum_{u,v,w}V(i+u,j+v,w)K(u,v,l,w)$</p>
<ul>
<li>Z: feature map特征图, 和V[输入的图]同样的dimension</li>
<li>$K(u,v,l,w)$: l代表输出的通道，w代表输入的通道，u/v表示行列</li>
</ul>
<p>Sparse interactions/稀疏交互：对于传统的神经网络，每个输出单元都与每个输入单元相互作用(全连接层)，但在处理数千或数百万像素的图像时，我们只选取有意义的特征(如边缘)，所以需要Sparse interactions，具体做法是让kernel小于input</p>
<p>尽管卷积网络中的直接连接非常稀疏，但更深层次中的单元可以间接连接到所有或大部分输入图像</p>
<p><img src="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20230113/yaimage.3qw2b944slk0.png" alt="yaimage"></p>
<h2 id="3-3-Pooling"><a href="#3-3-Pooling" class="headerlink" title="3.3 Pooling"></a>3.3 Pooling</h2><p>Padding/填充：在输入图像的边缘填充一些特定值，便于所有的边缘部分被kernel覆盖，保证输入图像和输出图像的尺寸相同</p>
<ul>
<li>Valid Padding：不padding</li>
<li>Zero padding：填成0</li>
</ul>
<p>Pooling：</p>
<ul>
<li>多个卷积核(特征)会导致计算量增大，为解决这个问题提出的概念，本质是缩小特征图。池化方式有：最大池化（选择被扫描区域内的最大值）和平均池化（取被扫描区域内的平均值）</li>
<li>A pooling function replaces the output of the net at a certain location with a summary statistic of the nearby outputs. 将output用一些统计值或者特征值表示</li>
<li>Max pooling: reports the maximum output within a rectangular neighborhood.</li>
<li>Other popular pooling functions: average of a rectangular neighborhood, L2 norm, weighted average based on the distance from the central pixel…</li>
<li>Invariance/不变性<ul>
<li>A stride of one pixel is applied in the input不会引起池化结果很大的改变</li>
<li>By pooling over the outputs of separately parametrized convolutions, the features can learn which transformations to become invariant</li>
</ul>
</li>
<li>可以处理inputs with different sizes的问题</li>
</ul>
<h2 id="3-4-CNN-architectures"><a href="#3-4-CNN-architectures" class="headerlink" title="3.4 CNN architectures"></a>3.4 CNN architectures</h2><p>卷积网络的典型层包括下面三个阶段:</p>
<p>Convolution layer: the layer performs several convolutions in parallel to produce a set of <em>linear activations</em>. </p>
<p>Detector layer: each linear activation is run through a <em>nonlinear activation function</em>. </p>
<p>Pooling layer: pooling function to modify the output of the layer further.</p>
<p><img src="C:\Users\nili990221\AppData\Roaming\Typora\typora-user-images\image-20230113233522114.png" alt="image-20230113233522114"></p>
</div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2023/01/31/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0_TP0_Plot&amp;Numpy/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">统计学习TP1</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2023/01/31/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0_CM5_Tree%20based%20methods/"><span class="level-item">统计学习5_CART</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://nililili7876.cn/2023/01/31/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0_CM6_Neural%20Networks/';
            this.page.identifier = '2023/01/31/统计学习_CM6_Neural Networks/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + '你来了啊' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20221029/ya微信图片_202112041747093.77tsun4ckc80.jpg" alt="7876"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">7876</p><p class="is-size-6 is-block">一个平平无奇的美女罢了</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Nantes France</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">78</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">30</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">34</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ppoffice" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#CM6-Neural-Networks1"><span class="level-left"><span class="level-item">1</span><span class="level-item">CM6_Neural Networks1</span></span></a></li><li><a class="level is-mobile" href="#0-Intro"><span class="level-left"><span class="level-item">2</span><span class="level-item">0. Intro</span></span></a></li><li><a class="level is-mobile" href="#1-Neural-Network"><span class="level-left"><span class="level-item">3</span><span class="level-item">1. Neural Network</span></span></a></li><li><a class="level is-mobile" href="#2-Feedforward-neural-network"><span class="level-left"><span class="level-item">4</span><span class="level-item">2. Feedforward neural network</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#2-1-One-hidden-layer-network"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">2.1 One-hidden layer network</span></span></a></li><li><a class="level is-mobile" href="#2-2-Multilayer-perceptron"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">2.2 Multilayer perceptron</span></span></a></li><li><a class="level is-mobile" href="#2-3-Training-Feed-Forward-NN"><span class="level-left"><span class="level-item">4.3</span><span class="level-item">2.3. Training Feed Forward NN</span></span></a></li></ul></li><li><a class="level is-mobile" href="#3-CNN"><span class="level-left"><span class="level-item">5</span><span class="level-item">3.CNN</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#3-1-Filters"><span class="level-left"><span class="level-item">5.1</span><span class="level-item">3.1 Filters</span></span></a></li><li><a class="level is-mobile" href="#3-2-Convolution"><span class="level-left"><span class="level-item">5.2</span><span class="level-item">3.2 Convolution</span></span></a></li><li><a class="level is-mobile" href="#3-3-Pooling"><span class="level-left"><span class="level-item">5.3</span><span class="level-item">3.3 Pooling</span></span></a></li><li><a class="level is-mobile" href="#3-4-CNN-architectures"><span class="level-left"><span class="level-item">5.4</span><span class="level-item">3.4 CNN architectures</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20221029/ya39f1ada59d3fa9d65440760f6b18c21.jpg" alt="Bienvenue en Planète 7876" height="28"></a><p class="is-size-7"><span>&copy; 2023 7876</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>