<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>GPU_TDs - Bienvenue en Planète 7876</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#3273dc"><meta name="application-name" content="Icaurs - Hexo Theme"><meta name="msapplication-TileImage" content="icons/touch-icon-iphone.png"><meta name="msapplication-TileColor" content="#3273dc"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Icaurs - Hexo Theme"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="144x144" href="icons/touch-icon-iphone.png"><link rel="apple-touch-icon" sizes="152x152" href="icons/touch-icon-ipad.png"><link rel="apple-touch-icon" sizes="72x72" href="icon/logo.ico"><link rel="apple-touch-icon" sizes="96x96" href="icon/logo.ico"><link rel="apple-touch-icon" sizes="128x128" href="icon/logo.ico"><link rel="apple-touch-icon" sizes="256x256" href="icon/logo.ico"><meta name="description" content="GPU_TDsTD整理"><meta property="og:type" content="blog"><meta property="og:title" content="111"><meta property="og:url" content="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20221029/ya%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20220917210647.jpg"><meta property="og:site_name" content="Bienvenue en Planète 7876"><meta property="og:description" content="GPU_TDsTD整理"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.statically.io/img/og_image.png"><meta property="article:published_time" content="2023-01-30T20:19:24.674Z"><meta property="article:modified_time" content="2023-01-30T20:50:27.021Z"><meta property="article:author" content="7876"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://cdn.statically.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://cdn.statically.io/gh/nililili7876/blog_pic@main/images/111.webp"},"headline":"GPU_TDs","image":["https://cdn.statically.io/img/og_image.png"],"datePublished":"2023-01-30T20:19:24.674Z","dateModified":"2023-01-30T20:50:27.021Z","author":{"@type":"Person","name":7876},"publisher":{"@type":"Organization","name":"Bienvenue en Planète 7876","logo":{"@type":"ImageObject","url":"https://cdn.statically.io/gh/nililili7876/blog_pic@main/20221029/ya39f1ada59d3fa9d65440760f6b18c21.jpg"}},"description":"GPU_TDsTD整理"}</script><link rel="canonical" href="http://nililili7876.cn/2023/01/31/GPU_TD/"><link rel="icon" href="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20221029/ya39f1ada59d3fa9d65440760f6b18c21.jpg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20221029/ya39f1ada59d3fa9d65440760f6b18c21.jpg" alt="Bienvenue en Planète 7876" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com">GitHub</a><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-01-30T20:19:24.674Z" title="2023/1/31 04:19:24">2023-01-30</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-01-30T20:50:27.021Z" title="2023/1/31 04:50:27">2023-01-30</time></span><span class="level-item"><a class="link-muted" href="/categories/NANTES/">NANTES</a><span> / </span><a class="link-muted" href="/categories/NANTES/EI2/">EI2</a><span> / </span><a class="link-muted" href="/categories/NANTES/EI2/EI2-2/">EI2_2</a><span> / </span><a class="link-muted" href="/categories/NANTES/EI2/EI2-2/GPU/">GPU</a></span><span class="level-item">32 minutes read (About 4832 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">GPU_TDs</h1><div class="content"><h1 id="GPU-TDs"><a href="#GPU-TDs" class="headerlink" title="GPU_TDs"></a>GPU_TDs</h1><p>TD整理</p>
<span id="more"></span>
<h1 id="TD1"><a href="#TD1" class="headerlink" title="TD1"></a>TD1</h1><ol>
<li><p>编译cu文件<code>nvcc -o outputFile inputFile.c</code>+启动可执行文件<code>./outputFile</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line">__device__ <span class="type">const</span> <span class="type">char</span> *STR = <span class="string">&quot;HELLO WORLD!&quot;</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">char</span> STR_LENGTH = <span class="number">12</span>;<span class="comment">//为什么不把这个也扔到device上==&gt;像int，float那种？</span></span><br><span class="line"></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">hello</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;%c\n&quot;</span>, STR[threadIdx.x % STR_LENGTH]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">void</span>)</span></span><br><span class="line">&#123;</span><br><span class="line">	<span class="type">int</span> num_threads = STR_LENGTH;</span><br><span class="line">	<span class="type">int</span> num_blocks = <span class="number">1</span>;</span><br><span class="line">	hello&lt;&lt;&lt;num_blocks,num_threads&gt;&gt;&gt;();</span><br><span class="line">	cudaDeviceSynchronize();</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>显示GPU的各种属性</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> *argv[])</span> &#123;</span><br><span class="line">  <span class="type">int</span> deviceCount;</span><br><span class="line">  cudaGetDeviceCount(&amp;deviceCount);</span><br><span class="line">  <span class="type">int</span> device;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">for</span> (device = <span class="number">0</span>; device &lt; deviceCount; ++device) &#123;</span><br><span class="line">    cudaDeviceProp deviceProp;</span><br><span class="line">    cudaGetDeviceProperties(&amp;deviceProp, device);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Device %d has compute capability %d.%d.\n&quot;</span>, device,</span><br><span class="line">           deviceProp.major, deviceProp.minor);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="TD2-一维向量加"><a href="#TD2-一维向量加" class="headerlink" title="TD2: 一维向量加"></a>TD2: 一维向量加</h1><p>一维向量加</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line">__global__</span><br><span class="line"><span class="type">void</span> <span class="title function_">vector_add</span><span class="params">(<span class="type">int</span> *a, <span class="type">int</span> *b, <span class="type">int</span> *c)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">//最关键的一行：index：第几块上的多少个线程</span></span><br><span class="line">	<span class="type">int</span> index = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">	c[index] = a[index] + b[index];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> N (2048*2048)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> THREADS_PER_BLOCK 512</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">	cudaEvent_t start, stop;</span><br><span class="line">	cudaEventCreate(&amp;start);</span><br><span class="line">	cudaEventCreate(&amp;stop);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> *a, *b, *c;</span><br><span class="line">	<span class="type">int</span> *d_a, *d_b, *d_c;</span><br><span class="line">	<span class="type">int</span> size = N * <span class="keyword">sizeof</span>( <span class="type">int</span> );</span><br><span class="line"></span><br><span class="line">	<span class="comment">//给device上的变量申请内存</span></span><br><span class="line">	cudaMalloc( (<span class="type">void</span> **) &amp;d_a, size );</span><br><span class="line">	cudaMalloc( (<span class="type">void</span> **) &amp;d_b, size );</span><br><span class="line">	cudaMalloc( (<span class="type">void</span> **) &amp;d_c, size );</span><br><span class="line">	<span class="comment">//给host上的变量申请内存</span></span><br><span class="line">	a = (<span class="type">int</span> *)<span class="built_in">malloc</span>( size );</span><br><span class="line">	b = (<span class="type">int</span> *)<span class="built_in">malloc</span>( size );</span><br><span class="line">	c = (<span class="type">int</span> *)<span class="built_in">malloc</span>( size );</span><br><span class="line">	</span><br><span class="line">    <span class="comment">//初始化</span></span><br><span class="line">	<span class="keyword">for</span>( <span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++ )</span><br><span class="line">	&#123;</span><br><span class="line">		a[i] = b[i] = i;</span><br><span class="line">		c[i] = <span class="number">0</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">    <span class="comment">//记录时间</span></span><br><span class="line">	cudaEventRecord(start, <span class="number">0</span>);</span><br><span class="line">	<span class="comment">//把host上的向量copy到device上</span></span><br><span class="line">	cudaMemcpy( d_a, a, size, cudaMemcpyHostToDevice );</span><br><span class="line">	cudaMemcpy( d_b, b, size, cudaMemcpyHostToDevice );</span><br><span class="line"></span><br><span class="line">	cudaEventRecord(stop, <span class="number">0</span>);</span><br><span class="line">	cudaEventSynchronize(stop);</span><br><span class="line">	</span><br><span class="line">    <span class="comment">//打印cudaMemcpy的时间</span></span><br><span class="line">	<span class="type">float</span> elapsedTime;</span><br><span class="line">	cudaEventElapsedTime(&amp;elapsedTime, start, stop);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;Time to copy %3.1f ms\n&quot;</span>, elapsedTime);</span><br><span class="line"></span><br><span class="line">	cudaEventRecord(start, <span class="number">0</span>);</span><br><span class="line">	<span class="comment">//launch the kernel on the GPU </span></span><br><span class="line">	<span class="comment">//参数是：bloc数【一定是向上取整的整数】和bloc里面的线程数</span></span><br><span class="line">	vector_add&lt;&lt;&lt; (N + (THREADS_PER_BLOCK<span class="number">-1</span>)) / THREADS_PER_BLOCK, THREADS_PER_BLOCK &gt;&gt;&gt;( d_a, d_b, d_c );</span><br><span class="line"></span><br><span class="line">	<span class="comment">//把结果copy回来</span></span><br><span class="line">	cudaMemcpy( c, d_c, size, cudaMemcpyDeviceToHost );</span><br><span class="line"></span><br><span class="line">	cudaEventRecord(stop, <span class="number">0</span>);</span><br><span class="line">	cudaEventSynchronize(stop);</span><br><span class="line">	<span class="comment">////kernel+cudaMemcpy回来的时间</span></span><br><span class="line">	cudaEventElapsedTime(&amp;elapsedTime, start, stop);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;Time to execute %3.1f ms\n&quot;</span>, elapsedTime);</span><br><span class="line">	cudaEventDestroy(start);</span><br><span class="line">	cudaEventDestroy(stop);</span><br><span class="line">	</span><br><span class="line">	<span class="built_in">printf</span>( <span class="string">&quot;c[0] = %d\n&quot;</span>,c[<span class="number">0</span>] );</span><br><span class="line">	<span class="built_in">printf</span>( <span class="string">&quot;c[%d] = %d\n&quot;</span>,N<span class="number">-1</span>, c[N<span class="number">-1</span>] );</span><br><span class="line"></span><br><span class="line">	<span class="comment">//清除缓存</span></span><br><span class="line">	<span class="built_in">free</span>(a);</span><br><span class="line">	<span class="built_in">free</span>(b);</span><br><span class="line">	<span class="built_in">free</span>(c);</span><br><span class="line">	cudaFree( d_a );</span><br><span class="line">	cudaFree( d_b );</span><br><span class="line">	cudaFree( d_c );</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="TD3-kernel-multidimension"><a href="#TD3-kernel-multidimension" class="headerlink" title="TD3: kernel multidimension"></a>TD3: kernel multidimension</h1><p>kernel multidimension</p>
<ol>
<li><p>Julia集</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"> <span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"> <span class="meta">#<span class="keyword">include</span> <span class="string">&quot;bitmap_image.hpp&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> DIM 1000</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">cuComplex</span> //负数的结构体--&gt;</span>C里面没有<span class="class"><span class="keyword">class</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="type">float</span>   r;</span><br><span class="line">    <span class="type">float</span>   i;</span><br><span class="line">    <span class="comment">//__device__:GPU执行，GPU使用</span></span><br><span class="line">    __device__ <span class="title function_">cuComplex</span><span class="params">( <span class="type">float</span> a, <span class="type">float</span> b )</span> : <span class="title function_">r</span><span class="params">(a)</span>, <span class="title function_">i</span><span class="params">(b)</span>  &#123;&#125;</span><br><span class="line">    __device__ <span class="type">float</span> <span class="title function_">magnitude2</span><span class="params">( <span class="type">void</span> )</span> &#123; <span class="keyword">return</span> r * r + i * i; &#125;</span><br><span class="line">    __device__ cuComplex operator*(<span class="type">const</span> cuComplex&amp; a)<span class="comment">//操作符重载是可以在结构体上的！</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> cuComplex(r*a.r - i*a.i, i*a.r + r*a.i);</span><br><span class="line">    &#125;</span><br><span class="line">    __device__ cuComplex operator+(<span class="type">const</span> cuComplex&amp; a) </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> cuComplex(r+a.r, i+a.i);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//因为在kernel里面被调用</span></span><br><span class="line">__device__ <span class="type">int</span> <span class="title function_">julia</span><span class="params">( <span class="type">int</span> x, <span class="type">int</span> y )</span> </span><br><span class="line">&#123; </span><br><span class="line">    <span class="type">const</span> <span class="type">float</span> scale = <span class="number">1.5</span>;</span><br><span class="line">    <span class="type">float</span> jx = scale * (<span class="type">float</span>)(DIM/<span class="number">2</span> - x)/(DIM/<span class="number">2</span>);</span><br><span class="line">    <span class="type">float</span> jy = scale * (<span class="type">float</span>)(DIM/<span class="number">2</span> - y)/(DIM/<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">    cuComplex <span class="title function_">c</span><span class="params">(<span class="number">-0.8</span>, <span class="number">0.156</span>)</span>;</span><br><span class="line">    cuComplex <span class="title function_">a</span><span class="params">(jx, jy)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> i = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (i=<span class="number">0</span>; i&lt;<span class="number">200</span>; i++) </span><br><span class="line">    &#123;</span><br><span class="line">        a = a * a + c;</span><br><span class="line">        <span class="keyword">if</span> (a.magnitude2() &gt; <span class="number">1000</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">kernel</span><span class="params">( <span class="type">unsigned</span> <span class="type">char</span> *ptr )</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> x = threadIdx.x + blockIdx.x * blockDim.x;<span class="comment">//x是col</span></span><br><span class="line">    <span class="type">int</span> y = threadIdx.y + blockIdx.y * blockDim.y;<span class="comment">//y是row</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (x &lt; DIM &amp;&amp; y &lt; DIM)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> offset = x + y * DIM;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> juliaValue = julia( x, y );</span><br><span class="line">        ptr[offset*<span class="number">3</span> + <span class="number">0</span>] = <span class="number">255</span> * juliaValue;<span class="comment">//可以有渐变效果？</span></span><br><span class="line">        ptr[offset*<span class="number">3</span> + <span class="number">1</span>] = <span class="number">0</span>;</span><br><span class="line">        ptr[offset*<span class="number">3</span> + <span class="number">2</span>] = <span class="number">0</span>;</span><br><span class="line">    &#125;        </span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">( <span class="type">void</span> )</span> </span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">//create event</span></span><br><span class="line">    cudaEvent_t start, stop;</span><br><span class="line">	cudaEventCreate(&amp;start);</span><br><span class="line">    cudaEventCreate(&amp;stop);</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> size = DIM*DIM*<span class="number">3</span>;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">char</span> *h_ptr = (<span class="type">unsigned</span> <span class="type">char</span> *)<span class="built_in">malloc</span>(size);</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">char</span> *d_ptr;</span><br><span class="line"></span><br><span class="line">    cudaMalloc((<span class="type">void</span>**)&amp;d_ptr, size);</span><br><span class="line">    </span><br><span class="line">    cudaEventRecord(start, <span class="number">0</span>);</span><br><span class="line">    <span class="comment">//这里噢</span></span><br><span class="line">    dim3 <span class="title function_">DimGrid</span> <span class="params">( <span class="built_in">ceil</span>(DIM/<span class="number">16.0</span>) , <span class="built_in">ceil</span>(DIM/<span class="number">16.0</span>)  , <span class="number">1</span>)</span>;</span><br><span class="line">    dim3 <span class="title function_">DimBlock</span> <span class="params">( <span class="number">16</span> , <span class="number">16</span> , <span class="number">1</span>)</span>;</span><br><span class="line">    kernel&lt;&lt;&lt; DimGrid,DimBlock &gt;&gt;&gt;( d_ptr );</span><br><span class="line">    <span class="comment">//event来搞时间</span></span><br><span class="line">    cudaEventRecord(stop, <span class="number">0</span>);</span><br><span class="line">	cudaEventSynchronize(stop);</span><br><span class="line">	<span class="type">float</span> elapsedTime;</span><br><span class="line">	cudaEventElapsedTime(&amp;elapsedTime, start, stop);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;Time to compute %3.1f ms\n&quot;</span>, elapsedTime);</span><br><span class="line"></span><br><span class="line">    cudaEventDestroy(start);</span><br><span class="line">    cudaEventDestroy(stop);</span><br><span class="line"></span><br><span class="line">    cudaMemcpy( h_ptr, d_ptr, size, cudaMemcpyDeviceToHost );</span><br><span class="line">    cudaFree(d_ptr);</span><br><span class="line">    </span><br><span class="line">	<span class="comment">// Write BMP image：非压缩图像格式，图像可以使用各种颜色深度，如1位，4位，8位和24位</span></span><br><span class="line">    <span class="comment">//在1位和4位深度中，图像是黑白或灰度图像，而在8位和24位深度中，图像是彩色图像。</span></span><br><span class="line">    bitmap_image <span class="title function_">img</span><span class="params">(DIM,DIM)</span>;</span><br><span class="line">    img.clear();</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> y = DIM<span class="number">-1</span>; y &gt;= <span class="number">0</span>; y--)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span> (<span class="type">int</span> x = DIM<span class="number">-1</span>; x &gt;= <span class="number">0</span>; x--)</span><br><span class="line">		&#123;</span><br><span class="line">            <span class="type">int</span> offset = x + y * DIM;</span><br><span class="line">            img.set_pixel(x, y, h_ptr[offset*<span class="number">3</span>], h_ptr[offset*<span class="number">3</span>+<span class="number">1</span>], h_ptr[offset*<span class="number">3</span>+<span class="number">2</span>]);            </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    img.save_image(<span class="string">&quot;test.bmp&quot;</span>);</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>矩阵乘法</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line"> <span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"> <span class="meta">#<span class="keyword">include</span> <span class="string">&lt;math.h&gt;</span></span></span><br><span class="line"> <span class="meta">#<span class="keyword">include</span> <span class="string">&lt;assert.h&gt;</span></span></span><br><span class="line"> </span><br><span class="line"> using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//都是只在main里面用的</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">initMatrix</span><span class="params">(<span class="type">float</span> *m, <span class="type">int</span> numRows, <span class="type">int</span> numCols)</span>;</span><br><span class="line"><span class="type">void</span> <span class="title function_">computeMatrixMulCPU</span><span class="params">(<span class="type">float</span> *A, <span class="type">float</span> *B, <span class="type">float</span> *C, <span class="type">int</span> numARows, <span class="type">int</span> numAColumns, <span class="type">int</span> numBRows, <span class="type">int</span> numBColumns)</span>;</span><br><span class="line"><span class="type">void</span> <span class="title function_">compareMatrix</span><span class="params">(<span class="type">float</span> *A, <span class="type">float</span> *B, <span class="type">int</span> numRows, <span class="type">int</span> numColumns)</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CREATE_CUDAEVENT cudaEvent_t start, stop; \</span></span><br><span class="line"><span class="meta">cudaEventCreate(&amp;start); \</span></span><br><span class="line"><span class="meta">cudaEventCreate(&amp;stop);</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> START_CUDAEVENT cudaEventRecord(start, 0);</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> STOP_AND_PRINT_CUDAEVENT(txt) cudaEventRecord(stop, 0);\</span></span><br><span class="line"><span class="meta">cudaEventSynchronize(stop);\</span></span><br><span class="line"><span class="meta">&#123;float elapsedTime;\</span></span><br><span class="line"><span class="meta">cudaEventElapsedTime(&amp;elapsedTime, start, stop);\</span></span><br><span class="line"><span class="meta">printf(<span class="string">&quot;Time to %s %3.1f ms\n&quot;</span>, #txt, elapsedTime);&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//ComputeC=A*B</span></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">sgemm</span><span class="params">(<span class="type">float</span> *A, <span class="type">float</span> *B, <span class="type">float</span> *C, <span class="type">int</span> numARows, <span class="type">int</span> numAColumns, <span class="type">int</span> numBRows, <span class="type">int</span> numBColumns)</span> &#123;</span><br><span class="line">    <span class="type">int</span> row = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">    <span class="type">int</span> col = blockIdx.x * blockDim.x + threadIdx.x; </span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (row &lt; numARows &amp;&amp; col &lt; numBColumns) &#123;</span><br><span class="line">        <span class="type">float</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> ii = <span class="number">0</span>; ii &lt; numAColumns; ii++) </span><br><span class="line">        &#123;</span><br><span class="line">            sum += A[row * numAColumns + ii] * B[ii * numBColumns + col];</span><br><span class="line">        &#125;</span><br><span class="line">        C[row * numBColumns + col] = sum;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> *argv[])</span></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">    CREATE_CUDAEVENT</span><br><span class="line"></span><br><span class="line">    <span class="comment">//这些是为了能在命令行输参数</span></span><br><span class="line">    <span class="comment">//调用：./matrix 10 30 30 40</span></span><br><span class="line">    <span class="type">int</span> numARows = atoi(argv[<span class="number">1</span>]); <span class="comment">// number of rows in the matrix A</span></span><br><span class="line">    <span class="type">int</span> numAColumns = atoi(argv[<span class="number">2</span>]); <span class="comment">// number of columns in the matrix A</span></span><br><span class="line">    <span class="type">int</span> numBRows = atoi(argv[<span class="number">3</span>]); <span class="comment">// number of rows in the matrix B</span></span><br><span class="line">    <span class="type">int</span> numBColumns = atoi(argv[<span class="number">4</span>]); <span class="comment">// number of columns in the matrix B</span></span><br><span class="line">    <span class="type">int</span> numCRows = numARows; <span class="comment">// number of rows in the matrix C</span></span><br><span class="line">    <span class="type">int</span> numCColumns = numBColumns; <span class="comment">// number of columns in the matrix C </span></span><br><span class="line">    assert(numAColumns == numBRows);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//host内存申请+初始化</span></span><br><span class="line">    <span class="type">float</span> *A = (<span class="type">float</span> *)<span class="built_in">malloc</span>(numARows*numAColumns*<span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    <span class="type">float</span> *B = (<span class="type">float</span> *)<span class="built_in">malloc</span>(numBRows*numBColumns*<span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    <span class="type">float</span> *C = (<span class="type">float</span> *)<span class="built_in">malloc</span>(numCRows*numCColumns*<span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    <span class="type">float</span> *hostC = (<span class="type">float</span> *)<span class="built_in">malloc</span>(numCRows*numCColumns*<span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    initMatrix(A, numARows, numAColumns);</span><br><span class="line">    initMatrix(B, numBRows, numBColumns);</span><br><span class="line"></span><br><span class="line">    START_CUDAEVENT</span><br><span class="line">    <span class="title function_">computeMatrixMulCPU</span><span class="params">(A, B, C, numARows, numAColumns, numBRows, numBColumns)</span>;</span><br><span class="line">    STOP_AND_PRINT_CUDAEVENT(compute CPU)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// CUDA申请device内存</span></span><br><span class="line">    <span class="type">float</span> *deviceA;</span><br><span class="line">    <span class="type">float</span> *deviceB;</span><br><span class="line">    <span class="type">float</span> *deviceC;</span><br><span class="line">    cudaMalloc((<span class="type">void</span> **)&amp;deviceA, numARows * numAColumns * <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    cudaMalloc((<span class="type">void</span> **)&amp;deviceB, numBRows * numBColumns * <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    cudaMalloc((<span class="type">void</span> **)&amp;deviceC, numCRows * numBColumns * <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line"></span><br><span class="line">    cudaMemcpy(deviceA, A, numARows * numAColumns * <span class="keyword">sizeof</span>(<span class="type">float</span>), cudaMemcpyHostToDevice);</span><br><span class="line">    cudaMemcpy(deviceB, B, numBRows * numBColumns * <span class="keyword">sizeof</span>(<span class="type">float</span>), cudaMemcpyHostToDevice);</span><br><span class="line">	<span class="comment">//这是什么神奇的用法：在 GPU 内存中将一段区域设置为特定的值。</span></span><br><span class="line">    cudaMemset(deviceC, <span class="number">0</span>, numCRows * numCColumns * <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//这里的gridDim也很有意思噢，其实也就是16啦</span></span><br><span class="line">    dim3 <span class="title function_">blockDim</span><span class="params">(<span class="number">16</span>, <span class="number">16</span>)</span>;</span><br><span class="line">    dim3 <span class="title function_">gridDim</span><span class="params">(<span class="built_in">ceil</span>(((<span class="type">float</span>)numBColumns) / blockDim.x), <span class="built_in">ceil</span>(((<span class="type">float</span>)numARows) / blockDim.y))</span>;</span><br><span class="line">    START_CUDAEVENT</span><br><span class="line">    sgemm&lt;&lt;&lt;gridDim, blockDim&gt;&gt;&gt;(deviceA, deviceB, deviceC, numARows, numAColumns, numBRows, numBColumns);</span><br><span class="line">    STOP_AND_PRINT_CUDAEVENT(compute GPU)</span><br><span class="line"></span><br><span class="line">    cudaMemcpy(hostC, deviceC, numARows * numBColumns * <span class="keyword">sizeof</span>(<span class="type">float</span>), cudaMemcpyDeviceToHost);</span><br><span class="line">    <span class="comment">// END CUDA PART</span></span><br><span class="line"></span><br><span class="line">    compareMatrix(C, hostC, numCRows, numCColumns);</span><br><span class="line"></span><br><span class="line">    cudaFree(deviceA);</span><br><span class="line">    cudaFree(deviceB);</span><br><span class="line">    cudaFree(deviceC);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">free</span>(A);</span><br><span class="line">    <span class="built_in">free</span>(B);</span><br><span class="line">    <span class="built_in">free</span>(C);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">initMatrix</span><span class="params">(<span class="type">float</span> *m, <span class="type">int</span> numRows, <span class="type">int</span> numCols)</span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;numRows; i++)&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j=<span class="number">0</span>; j&lt;numCols; j++)&#123;</span><br><span class="line">            m[i*numCols+j] = <span class="built_in">sin</span>(i*numCols+j);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">computeMatrixMulCPU</span><span class="params">(</span></span><br><span class="line"><span class="params">    <span class="type">float</span> *A, <span class="type">float</span> *B, <span class="type">float</span> *C,</span></span><br><span class="line"><span class="params">    <span class="type">int</span> numARows, <span class="type">int</span> numAColumns,</span></span><br><span class="line"><span class="params">    <span class="type">int</span> numBRows, <span class="type">int</span> numBColumns</span></span><br><span class="line"><span class="params">)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> row = <span class="number">0</span>; row &lt; numARows; row++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> col = <span class="number">0</span>; col &lt; numBColumns; col++)</span><br><span class="line">        &#123;   </span><br><span class="line">            C[row * numBColumns + col] = <span class="number">0.0</span>;         </span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> n = <span class="number">0</span>; n &lt; numAColumns; n++)</span><br><span class="line">            &#123;</span><br><span class="line">                C[row * numBColumns + col] += A[row * numAColumns + n] * B[n * numBColumns + col];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">compareMatrix</span><span class="params">(<span class="type">float</span> *A, <span class="type">float</span> *B, <span class="type">int</span> numRows, <span class="type">int</span> numColumns)</span>&#123;</span><br><span class="line">    <span class="type">float</span> sum = <span class="number">0.0</span>;</span><br><span class="line">    <span class="type">float</span> max = <span class="number">0.0</span>;</span><br><span class="line">    <span class="type">float</span> min = <span class="number">10.0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> row = <span class="number">0</span>; row &lt; numRows; row++)&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> col = <span class="number">0</span>; col &lt; numColumns; col++)&#123;    </span><br><span class="line">            <span class="type">float</span> diff = A[row*numColumns+col] - B[row*numColumns+col];</span><br><span class="line">            <span class="keyword">if</span> (diff &gt; max) max = diff;</span><br><span class="line">            <span class="keyword">if</span> (diff &lt; min) min = diff;</span><br><span class="line">            sum += diff;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;mean: &quot;</span> &lt;&lt; sum / (numRows*numColumns) &lt;&lt; <span class="string">&quot; max: &quot;</span> &lt;&lt; max &lt;&lt; <span class="string">&quot; min: &quot;</span> &lt;&lt; min &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="TD4-shared"><a href="#TD4-shared" class="headerlink" title="TD4: __shared__"></a>TD4: <code>__shared__</code></h1><ol>
<li><p>utiliser des synchronisation</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;bitmap_image.hpp&quot;</span></span></span><br><span class="line">   </span><br><span class="line"><span class="meta">#<span class="keyword">define</span> DIM 1024</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> PI 3.1415926535897932f</span></span><br><span class="line"></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">kernel</span><span class="params">( <span class="type">unsigned</span> <span class="type">char</span> *ptr )</span> </span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// map from threadIdx/BlockIdx to pixel position</span></span><br><span class="line">    <span class="type">int</span> x = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="type">int</span> y = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">    <span class="comment">//我们最常见的是x+y*witdth</span></span><br><span class="line">    <span class="comment">//这里的blockDim和gridDim在干啥哦</span></span><br><span class="line">    <span class="comment">//所以是blockDim.x * gridDim.x=width吗？？？？为什么？？？</span></span><br><span class="line">    <span class="comment">//是的！！！！！！</span></span><br><span class="line">    <span class="comment">//blockDim.x: 线程块的大小，gridDim.x: 整个网格的分块情况</span></span><br><span class="line">    <span class="type">int</span> offset = x + y * blockDim.x * gridDim.x;</span><br><span class="line"></span><br><span class="line">    __shared__ <span class="type">float</span> shared[<span class="number">16</span>][<span class="number">16</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// now calculate the value at that position</span></span><br><span class="line">    <span class="type">const</span> <span class="type">float</span> period = <span class="number">128.0f</span>;</span><br><span class="line"></span><br><span class="line">    shared[threadIdx.x][threadIdx.y] =</span><br><span class="line">            <span class="number">255</span> * (sinf(x*<span class="number">2.0f</span>*PI/ period) + <span class="number">1.0f</span>) *</span><br><span class="line">                  (sinf(y*<span class="number">2.0f</span>*PI/ period) + <span class="number">1.0f</span>) / <span class="number">4.0f</span>;</span><br><span class="line">    __syncthreads ();<span class="comment">//确保块中的每个线程在__syncthreads()之前完成了它的指令</span></span><br><span class="line"></span><br><span class="line">    ptr[offset*<span class="number">3</span> + <span class="number">0</span>] = shared[<span class="number">15</span>-threadIdx.x][<span class="number">15</span>-threadIdx.y];</span><br><span class="line">    ptr[offset*<span class="number">3</span> + <span class="number">1</span>] = <span class="number">0</span>;</span><br><span class="line">    ptr[offset*<span class="number">3</span> + <span class="number">2</span>] = <span class="number">255</span>;</span><br><span class="line">     __syncthreads ();</span><br><span class="line">   </span><br><span class="line">&#125;</span><br><span class="line">   </span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">( <span class="type">void</span> )</span> </span><br><span class="line">&#123;</span><br><span class="line">   <span class="type">int</span> size = DIM*DIM*<span class="number">3</span>*<span class="keyword">sizeof</span>(<span class="type">unsigned</span> <span class="type">char</span>);</span><br><span class="line">   <span class="type">unsigned</span> <span class="type">char</span> *h_ptr = (<span class="type">unsigned</span> <span class="type">char</span> *)<span class="built_in">malloc</span>(size);</span><br><span class="line">   <span class="type">unsigned</span> <span class="type">char</span> *d_ptr;</span><br><span class="line">   </span><br><span class="line">   cudaMalloc( (<span class="type">void</span>**)&amp;d_ptr, size );</span><br><span class="line"></span><br><span class="line">   <span class="comment">//这里的dim3只传了x，y，意思是z取默认值1</span></span><br><span class="line">   dim3   <span class="title function_">grids</span><span class="params">(DIM/<span class="number">16</span>,DIM/<span class="number">16</span>)</span>;</span><br><span class="line">   dim3   <span class="title function_">threads</span><span class="params">(<span class="number">16</span>,<span class="number">16</span>)</span>;</span><br><span class="line">   kernel&lt;&lt;&lt;grids,threads&gt;&gt;&gt;( d_ptr );</span><br><span class="line">   </span><br><span class="line">   cudaMemcpy( h_ptr, d_ptr, size, cudaMemcpyDeviceToHost );</span><br><span class="line">   </span><br><span class="line">   bitmap_image <span class="title function_">img</span><span class="params">(DIM,DIM)</span>;</span><br><span class="line">   img.clear();</span><br><span class="line">   <span class="keyword">for</span> (<span class="type">int</span> y = DIM<span class="number">-1</span>; y &gt;= <span class="number">0</span>; y--)</span><br><span class="line">&#123;</span><br><span class="line">	<span class="keyword">for</span> (<span class="type">int</span> x = DIM<span class="number">-1</span>; x &gt;= <span class="number">0</span>; x--)</span><br><span class="line">	&#123;</span><br><span class="line">           <span class="type">int</span> offset = x + y * DIM;</span><br><span class="line">           img.set_pixel(x, y, h_ptr[offset*<span class="number">3</span>], h_ptr[offset*<span class="number">3</span>+<span class="number">1</span>], h_ptr[offset*<span class="number">3</span>+<span class="number">2</span>]);            </span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   img.save_image(<span class="string">&quot;test.bmp&quot;</span>);</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Stencil 1D</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> N 4000000</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> RADIUS 5</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> THREADS_PER_BLOCK 512</span></span><br><span class="line"></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">stencil_1d</span><span class="params">(<span class="type">int</span> n, <span class="type">double</span> *in, <span class="type">double</span> *out)</span></span><br><span class="line">&#123;</span><br><span class="line"> <span class="comment">//__shared__ 做用在bloc上，可以在同一个block里面的线程里面共享</span></span><br><span class="line"> __shared__ <span class="type">double</span> tmp[THREADS_PER_BLOCK + <span class="number">2</span>*RADIUS];</span><br><span class="line"> <span class="type">int</span> gindex = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line"> <span class="type">int</span> lindex = threadIdx.x + RADIUS;<span class="comment">//？？？？？？？</span></span><br><span class="line">    </span><br><span class="line"> <span class="keyword">if</span>( gindex &lt; n )</span><br><span class="line"> &#123;</span><br><span class="line">   <span class="comment">//加载tmp--&gt;感觉用shared的话在代码上重点就在这里+下面计算用shared而不是in</span></span><br><span class="line">   tmp[lindex] = in[gindex];</span><br><span class="line">   __syncthreads();</span><br><span class="line"></span><br><span class="line">  <span class="comment">//如果在边界上</span></span><br><span class="line">  <span class="keyword">if</span>( gindex &lt; RADIUS || gindex &gt;= (n - RADIUS) ) </span><br><span class="line">  &#123;</span><br><span class="line">    <span class="comment">//out[i] = i*11</span></span><br><span class="line">    out[gindex] = (<span class="type">double</span>) gindex * ((<span class="type">double</span>)RADIUS*<span class="number">2</span> + <span class="number">1</span>) ;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//不在边界往两边做累加</span></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">  &#123; </span><br><span class="line">    <span class="type">double</span> result = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">for</span>( <span class="type">int</span> i = gindex-(RADIUS); i &lt;= gindex+(RADIUS); i++ ) </span><br><span class="line">    &#123;</span><br><span class="line">      result += tmp[i];</span><br><span class="line">    &#125; </span><br><span class="line">    out[gindex] = result;</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="type">double</span> *in, *out;</span><br><span class="line">  <span class="type">double</span> *d_in, *d_out;</span><br><span class="line">  <span class="type">int</span> size = N * <span class="keyword">sizeof</span>( <span class="type">double</span> );</span><br><span class="line"></span><br><span class="line">  cudaMalloc( (<span class="type">void</span> **) &amp;d_in, size );</span><br><span class="line">  cudaMalloc( (<span class="type">void</span> **) &amp;d_out, size );</span><br><span class="line">  in = (<span class="type">double</span> *)<span class="built_in">malloc</span>( size );</span><br><span class="line">  out = (<span class="type">double</span> *)<span class="built_in">malloc</span>( size );</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span>( <span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++ )</span><br><span class="line">  &#123;</span><br><span class="line">    in[i] = (<span class="type">double</span>) i;</span><br><span class="line">    out[i] = <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  cudaMemcpy( d_in, in, size, cudaMemcpyHostToDevice );</span><br><span class="line">  <span class="comment">//这里吧d_out顺手初始化了0了</span></span><br><span class="line">  cudaMemset( d_out, <span class="number">0</span>, size );</span><br><span class="line"></span><br><span class="line">  <span class="comment">//这里不是一维的vec，干嘛要用dim3来处理--&gt;可以滴</span></span><br><span class="line">  dim3 <span class="title function_">threads</span><span class="params">( THREADS_PER_BLOCK, <span class="number">1</span>, <span class="number">1</span>)</span>;</span><br><span class="line">  dim3 <span class="title function_">blocks</span><span class="params">( N / THREADS_PER_BLOCK+<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span>;</span><br><span class="line">  <span class="comment">//时间</span></span><br><span class="line">  cudaEvent_t start, stop;</span><br><span class="line">  cudaEventCreate( &amp;start );</span><br><span class="line">  cudaEventCreate( &amp;stop );</span><br><span class="line">  cudaEventRecord( start, <span class="number">0</span> );</span><br><span class="line">  <span class="comment">//kernel</span></span><br><span class="line">  stencil_1d&lt;&lt;&lt; blocks, threads &gt;&gt;&gt;( N, d_in, d_out );</span><br><span class="line">  <span class="comment">//天呐！</span></span><br><span class="line">  cudaDeviceSynchronize();</span><br><span class="line">  <span class="comment">//时间</span></span><br><span class="line">  cudaEventRecord( stop, <span class="number">0</span> );</span><br><span class="line">  cudaEventSynchronize( stop );</span><br><span class="line">  <span class="type">float</span> elapsedTime;</span><br><span class="line">  cudaEventElapsedTime( &amp;elapsedTime, start, stop );</span><br><span class="line"></span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Total time for %d elements was %f ms\n&quot;</span>, N, elapsedTime );</span><br><span class="line">  <span class="comment">//把结果粘回来</span></span><br><span class="line">  cudaMemcpy( out, d_out, size, cudaMemcpyDeviceToHost );</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span>( <span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++ )</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">if</span>( in[i]*((<span class="type">double</span>)RADIUS*<span class="number">2</span>+<span class="number">1</span>) != out[i] ) <span class="comment">//要保证in[i]*11==out[i]，什么毛病</span></span><br><span class="line">    &#123;</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">&quot;error in element %d in = %f out %f\n&quot;</span>,i,in[i],out[i] );</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">&quot;FAIL\n&quot;</span>);</span><br><span class="line">      <span class="keyword">goto</span> end;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line"></span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;PASS\n&quot;</span>);</span><br><span class="line">  end;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">free</span>(in);</span><br><span class="line">  <span class="built_in">free</span>(out);</span><br><span class="line">  cudaFree( d_in );</span><br><span class="line">  cudaFree( d_out );</span><br><span class="line"></span><br><span class="line">  cudaDeviceReset();</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
</li>
<li><p>矩阵乘avec tuile</p>
<p>只展示kernel部分</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> TILE_WIDTH 16</span></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">sgemm</span><span class="params">(<span class="type">float</span> *A, <span class="type">float</span> *B, <span class="type">float</span> *C, <span class="type">int</span> numARows, <span class="type">int</span> numAColumns, <span class="type">int</span> numBRows, <span class="type">int</span> numBColumns)</span> </span><br><span class="line">&#123;</span><br><span class="line">    __shared__ <span class="type">float</span> ds_M[TILE_WIDTH][TILE_WIDTH];</span><br><span class="line">    __shared__ <span class="type">float</span> ds_N[TILE_WIDTH][TILE_WIDTH];</span><br><span class="line">    <span class="type">int</span> bx = blockIdx.x, by = blockIdx.y, tx = threadIdx.x, ty = threadIdx.y;</span><br><span class="line">    <span class="comment">//平时我们这里by乘的应该是blockDim，但在这里都是16，也行</span></span><br><span class="line">    <span class="type">int</span> row = by * TILE_WIDTH + ty, col = bx * TILE_WIDTH + tx;</span><br><span class="line">    <span class="type">float</span> Pvalue = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> m = <span class="number">0</span>; m &lt; (numAColumns - <span class="number">1</span>) / TILE_WIDTH + <span class="number">1</span>; ++m)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//给共享内存ds_M赋值</span></span><br><span class="line">        <span class="keyword">if</span> (row &lt; numARows &amp;&amp; m * TILE_WIDTH + tx &lt; numAColumns)</span><br><span class="line">            <span class="comment">//又出现了，这个奇怪的下标</span></span><br><span class="line">            ds_M[ty][tx] = A[row * numAColumns + m * TILE_WIDTH + tx];</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            ds_M[ty][tx] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span> (col &lt; numBColumns &amp;&amp; m * TILE_WIDTH + ty &lt; numBRows)</span><br><span class="line">            <span class="comment">//奇怪下标*2</span></span><br><span class="line">            ds_N[ty][tx] = B[(m * TILE_WIDTH + ty) * numBColumns + col];</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            ds_N[ty][tx] = <span class="number">0</span>;</span><br><span class="line">        __syncthreads();</span><br><span class="line">		</span><br><span class="line">        <span class="comment">//用共享内存来计算</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; TILE_WIDTH; ++k)</span><br><span class="line">            Pvalue += ds_M[ty][k] * ds_N[k][tx];</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//赋值</span></span><br><span class="line">    <span class="keyword">if</span> (row &lt; numARows &amp;&amp; col &lt; numBColumns)</span><br><span class="line">        C[row * numBColumns + col] = Pvalue;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment">/*int row = blockIdx.y * blockDim.y + threadIdx.y;</span></span><br><span class="line"><span class="comment">    int col = blockIdx.x * blockDim.x + threadIdx.x; </span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    if (row &lt; numARows &amp;&amp; col &lt; numBColumns) &#123;</span></span><br><span class="line"><span class="comment">        float sum = 0;</span></span><br><span class="line"><span class="comment">        for (int ii = 0; ii &lt; numAColumns; ii++) &#123;</span></span><br><span class="line"><span class="comment">            sum += A[row * numAColumns + ii] * B[ii * numBColumns + col];</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">        C[row * numBColumns + col] = sum;</span></span><br><span class="line"><span class="comment">    &#125;*/</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="TD5-内存"><a href="#TD5-内存" class="headerlink" title="TD5: 内存"></a>TD5: 内存</h1><p>fixed/pinned: <code>cudaHostAlloc((void **) &amp;h_ap, size, cudaHostAllocDefault);</code></p>
<p>unified: <code>cudaMallocManaged( (void **) &amp;a_um, size );</code>,这个不用cudaMemcpy，但有在kernel后面+<code>cudaDeviceSynchronize();</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CREATE_CUDAEVENT cudaEvent_t start, stop; \</span></span><br><span class="line"><span class="meta">cudaEventCreate(&amp;start); \</span></span><br><span class="line"><span class="meta">cudaEventCreate(&amp;stop);</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> START_CUDAEVENT cudaEventRecord(start, 0);</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> STOP_AND_PRINT_CUDAEVENT(txt) cudaEventRecord(stop, 0);\</span></span><br><span class="line"><span class="meta">cudaEventSynchronize(stop);\</span></span><br><span class="line"><span class="meta">&#123;float elapsedTime;\</span></span><br><span class="line"><span class="meta">cudaEventElapsedTime(&amp;elapsedTime, start, stop);\</span></span><br><span class="line"><span class="meta">printf(<span class="string">&quot;Time to %s %3.1f ms\n&quot;</span>, #txt, elapsedTime);&#125;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">vector_add</span><span class="params">(<span class="type">int</span> *a, <span class="type">int</span> *b, <span class="type">int</span> *c)</span></span><br><span class="line">&#123;</span><br><span class="line">	<span class="type">int</span> index = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">	c[index] = a[index] + b[index];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">vector_add_UM</span><span class="params">(<span class="type">int</span> *a, <span class="type">int</span> *b, <span class="type">int</span> *c)</span></span><br><span class="line">&#123;</span><br><span class="line">	<span class="type">int</span> index = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">	c[index] = a[index] + b[index];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> N (2048*2048)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> THREADS_PER_BLOCK 512</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">	CREATE_CUDAEVENT;</span><br><span class="line">	<span class="type">int</span> size = N * <span class="keyword">sizeof</span>( <span class="type">int</span> );</span><br><span class="line"></span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;&gt;&gt;&gt; Results for MemCopy\n&quot;</span>);</span><br><span class="line">	<span class="type">int</span> *h_a, *h_b, *h_c;</span><br><span class="line">	<span class="type">int</span> *d_a, *d_b, *d_c;</span><br><span class="line"></span><br><span class="line">	START_CUDAEVENT</span><br><span class="line">	h_a = (<span class="type">int</span> *)<span class="built_in">malloc</span>( size );</span><br><span class="line">	h_b = (<span class="type">int</span> *)<span class="built_in">malloc</span>( size );</span><br><span class="line">	h_c = (<span class="type">int</span> *)<span class="built_in">malloc</span>( size );</span><br><span class="line">	STOP_AND_PRINT_CUDAEVENT([Classical] host allocation)</span><br><span class="line"></span><br><span class="line">	START_CUDAEVENT</span><br><span class="line">	<span class="title function_">for</span><span class="params">( <span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++ )</span></span><br><span class="line">	&#123;</span><br><span class="line">		h_a[i] = h_b[i] = i;</span><br><span class="line">		h_c[i] = <span class="number">0</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	STOP_AND_PRINT_CUDAEVENT([Classical] Initialize)</span><br><span class="line"></span><br><span class="line">	START_CUDAEVENT</span><br><span class="line">	<span class="title function_">cudaMalloc</span><span class="params">( (<span class="type">void</span> **) &amp;d_a, size )</span>;</span><br><span class="line">	cudaMalloc( (<span class="type">void</span> **) &amp;d_b, size );</span><br><span class="line">	cudaMalloc( (<span class="type">void</span> **) &amp;d_c, size );</span><br><span class="line">	STOP_AND_PRINT_CUDAEVENT([Classical] device allocation)</span><br><span class="line"></span><br><span class="line">	START_CUDAEVENT</span><br><span class="line">	<span class="title function_">cudaMemcpy</span><span class="params">( d_a, h_a, size, cudaMemcpyHostToDevice )</span>;</span><br><span class="line">	cudaMemcpy( d_b, h_b, size, cudaMemcpyHostToDevice );	</span><br><span class="line">	STOP_AND_PRINT_CUDAEVENT([Classical] MemCopy Host to Device)</span><br><span class="line"></span><br><span class="line">	START_CUDAEVENT</span><br><span class="line">	vector_add&lt;&lt;&lt; (N + (THREADS_PER_BLOCK<span class="number">-1</span>)) / THREADS_PER_BLOCK, THREADS_PER_BLOCK &gt;&gt;&gt;( d_a, d_b, d_c );</span><br><span class="line">	STOP_AND_PRINT_CUDAEVENT([Classical] execution)</span><br><span class="line"></span><br><span class="line">	START_CUDAEVENT</span><br><span class="line">	<span class="title function_">cudaMemcpy</span><span class="params">( h_c, d_c, size, cudaMemcpyDeviceToHost )</span>;</span><br><span class="line">	STOP_AND_PRINT_CUDAEVENT([Classical] MemCopy Device to Host)</span><br><span class="line"></span><br><span class="line">	<span class="built_in">free</span>(h_a);</span><br><span class="line">	<span class="built_in">free</span>(h_b);</span><br><span class="line">	<span class="built_in">free</span>(h_c);</span><br><span class="line">	cudaFree( d_a );</span><br><span class="line">	cudaFree( d_b );</span><br><span class="line">	cudaFree( d_c );</span><br><span class="line">	</span><br><span class="line">    <span class="comment">//pinned memory：cudaHostAlloc</span></span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;\n&gt;&gt;&gt; Results for Pinned Memory\n&quot;</span>);</span><br><span class="line">	<span class="type">int</span> *h_ap, *h_bp, *h_cp;</span><br><span class="line">	<span class="type">int</span> *d_ap, *d_bp, *d_cp;</span><br><span class="line"></span><br><span class="line">	START_CUDAEVENT</span><br><span class="line">	<span class="title function_">cudaHostAlloc</span><span class="params">((<span class="type">void</span> **) &amp;h_ap, size, cudaHostAllocDefault)</span>;</span><br><span class="line">	cudaHostAlloc((<span class="type">void</span> **) &amp;h_bp, size, cudaHostAllocDefault);</span><br><span class="line">	cudaHostAlloc((<span class="type">void</span> **) &amp;h_cp, size, cudaHostAllocDefault);</span><br><span class="line">	STOP_AND_PRINT_CUDAEVENT([Pinned] host allocation)</span><br><span class="line">	</span><br><span class="line">	START_CUDAEVENT</span><br><span class="line">	<span class="title function_">for</span><span class="params">( <span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++ )</span></span><br><span class="line">	&#123;</span><br><span class="line">		h_ap[i] = h_bp[i] = i;</span><br><span class="line">		h_cp[i] = <span class="number">0</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	STOP_AND_PRINT_CUDAEVENT([Pinned] initialize)</span><br><span class="line"></span><br><span class="line">	START_CUDAEVENT</span><br><span class="line">	<span class="title function_">cudaMalloc</span><span class="params">( (<span class="type">void</span> **) &amp;d_ap, size )</span>;</span><br><span class="line">	cudaMalloc( (<span class="type">void</span> **) &amp;d_bp, size );</span><br><span class="line">	cudaMalloc( (<span class="type">void</span> **) &amp;d_cp, size );</span><br><span class="line">	STOP_AND_PRINT_CUDAEVENT([Pinned] device allocation)</span><br><span class="line"></span><br><span class="line">	START_CUDAEVENT</span><br><span class="line">	<span class="title function_">cudaMemcpy</span><span class="params">( d_ap, h_ap, size, cudaMemcpyHostToDevice )</span>;</span><br><span class="line">	cudaMemcpy( d_bp, h_bp, size, cudaMemcpyHostToDevice );	</span><br><span class="line">	STOP_AND_PRINT_CUDAEVENT([Pinned] MemCopy Host to Device)</span><br><span class="line"></span><br><span class="line">	START_CUDAEVENT</span><br><span class="line">	vector_add&lt;&lt;&lt; (N + (THREADS_PER_BLOCK<span class="number">-1</span>)) / THREADS_PER_BLOCK, THREADS_PER_BLOCK &gt;&gt;&gt;( d_ap, d_bp, d_cp );</span><br><span class="line">	STOP_AND_PRINT_CUDAEVENT([Pinned] execution)</span><br><span class="line"></span><br><span class="line">	START_CUDAEVENT</span><br><span class="line">	<span class="title function_">cudaMemcpy</span><span class="params">( h_cp, d_cp, size, cudaMemcpyDeviceToHost )</span>;</span><br><span class="line">	STOP_AND_PRINT_CUDAEVENT([Pinned] MemCopy Device to Host)</span><br><span class="line"></span><br><span class="line">	cudaFreeHost(h_ap);</span><br><span class="line">	cudaFreeHost(h_bp);</span><br><span class="line">	cudaFreeHost(h_cp);</span><br><span class="line">	cudaFree( d_ap );</span><br><span class="line">	cudaFree( d_bp );</span><br><span class="line">	cudaFree( d_cp );</span><br><span class="line">	</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Unified Memory：cudaMallocManaged</span></span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;\n&gt;&gt;&gt; Results for Unified Memory\n&quot;</span>);</span><br><span class="line">    <span class="type">int</span> *a_um, *b_um, *c_um;	</span><br><span class="line"></span><br><span class="line">	START_CUDAEVENT</span><br><span class="line">	<span class="title function_">cudaMallocManaged</span><span class="params">( (<span class="type">void</span> **) &amp;a_um, size )</span>;</span><br><span class="line">	cudaMallocManaged( (<span class="type">void</span> **) &amp;b_um, size );</span><br><span class="line">	cudaMallocManaged( (<span class="type">void</span> **) &amp;c_um, size );</span><br><span class="line">	STOP_AND_PRINT_CUDAEVENT([Unified Memory] memory allocation)</span><br><span class="line"></span><br><span class="line">	START_CUDAEVENT</span><br><span class="line">	<span class="title function_">for</span><span class="params">( <span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++ )</span></span><br><span class="line">	&#123;</span><br><span class="line">		a_um[i] = b_um[i] = i;</span><br><span class="line">		c_um[i] = <span class="number">0</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	STOP_AND_PRINT_CUDAEVENT([Unified Memory] initialize)</span><br><span class="line"></span><br><span class="line">	START_CUDAEVENT</span><br><span class="line">	<span class="title function_">STOP_AND_PRINT_CUDAEVENT</span><span class="params">([Unified Memory] MemCopy Host to Device)</span></span><br><span class="line"></span><br><span class="line">	START_CUDAEVENT</span><br><span class="line">	vector_add_UM&lt;&lt;&lt; <span class="params">(N + (THREADS_PER_BLOCK<span class="number">-1</span>))</span> / THREADS_PER_BLOCK, THREADS_PER_BLOCK &gt;&gt;&gt;<span class="params">( a_um, b_um, c_um )</span>;</span><br><span class="line">	cudaDeviceSynchronize();</span><br><span class="line">	STOP_AND_PRINT_CUDAEVENT([Unified Memory] execution)</span><br><span class="line"></span><br><span class="line">	START_CUDAEVENT</span><br><span class="line">	<span class="title function_">STOP_AND_PRINT_CUDAEVENT</span><span class="params">([Unified Memory] MemCopy Device to Host)</span></span><br><span class="line"></span><br><span class="line">	<span class="title function_">cudaFree</span><span class="params">( a_um )</span>;</span><br><span class="line">	cudaFree( b_um );</span><br><span class="line">	cudaFree( c_um );</span><br><span class="line"></span><br><span class="line">	cudaEventDestroy(start);</span><br><span class="line">	cudaEventDestroy(stop);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<h1 id="TD6-coalesced"><a href="#TD6-coalesced" class="headerlink" title="TD6: coalesced"></a>TD6: coalesced</h1><p>代码中涉及到的一些概念</p>
<ul>
<li>Offset表示在内存中的偏移量，它指定了访问数据的起始位置</li>
<li>“Stride” 指的是在访问内存时，每次跨过的字节数。在 CUDA 程序中，当访问一维数组时，如果每次访问相邻元素，则 stride 为 1。当访问二维数组时，如果在行之间跨越，则 stride 是该行的字节数。</li>
<li>“Bandwidth” 指的是单位时间内能够访问的数据量。在 CUDA 中，它主要是指 GPU 和 CPU 之间的数据传输速率，包括从 CPU 到 GPU 的数据传输和从 GPU 到 CPU 的数据传输。</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;assert.h&gt;</span></span></span><br><span class="line"> </span><br><span class="line"> <span class="comment">// Convenience function for checking CUDA runtime API results</span></span><br><span class="line"> <span class="comment">// can be wrapped around any runtime API call. No-op in release builds.</span></span><br><span class="line"> <span class="keyword">inline</span></span><br><span class="line"> cudaError_t <span class="title function_">checkCuda</span><span class="params">(cudaError_t result)</span></span><br><span class="line"> &#123;</span><br><span class="line"> <span class="meta">#<span class="keyword">if</span> defined(DEBUG) || defined(_DEBUG)</span></span><br><span class="line">   <span class="keyword">if</span> (result != cudaSuccess) &#123;</span><br><span class="line">     <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">&quot;CUDA Runtime Error: %s\n&quot;</span>, cudaGetErrorString(result));</span><br><span class="line">     assert(result == cudaSuccess);</span><br><span class="line">   &#125;</span><br><span class="line"> <span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">   <span class="keyword">return</span> result;</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> template &lt;typename T&gt;</span><br><span class="line"> __global__ <span class="type">void</span> <span class="title function_">offset</span><span class="params">(T* a, <span class="type">int</span> s)</span></span><br><span class="line"> &#123;</span><br><span class="line">   <span class="type">int</span> i = blockDim.x * blockIdx.x + threadIdx.x + s;</span><br><span class="line">   a[i] = a[i] + <span class="number">1</span>;</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> template &lt;typename T&gt;</span><br><span class="line"> __global__ <span class="type">void</span> <span class="title function_">stride</span><span class="params">(T* a, <span class="type">int</span> s)</span></span><br><span class="line"> &#123;</span><br><span class="line">   <span class="type">int</span> i = (blockDim.x * blockIdx.x + threadIdx.x) * s;</span><br><span class="line">   a[i] = a[i] + <span class="number">1</span>;</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> template &lt;typename T&gt;</span><br><span class="line"> <span class="type">void</span> <span class="title function_">runTest</span><span class="params">(<span class="type">int</span> deviceId, <span class="type">int</span> nMB)</span></span><br><span class="line"> &#123;</span><br><span class="line">   <span class="type">int</span> blockSize = <span class="number">256</span>;</span><br><span class="line">   <span class="type">float</span> ms;</span><br><span class="line"> </span><br><span class="line">   T *d_a;</span><br><span class="line">   cudaEvent_t startEvent, stopEvent;</span><br><span class="line">     </span><br><span class="line">   <span class="type">int</span> n = nMB*<span class="number">1024</span>*<span class="number">1024</span>/<span class="keyword">sizeof</span>(T);</span><br><span class="line"> </span><br><span class="line">   <span class="comment">// NB:  d_a(33*nMB) for stride case</span></span><br><span class="line">   checkCuda( cudaMalloc(&amp;d_a, n * <span class="number">33</span> * <span class="keyword">sizeof</span>(T)) );</span><br><span class="line"> </span><br><span class="line">   checkCuda( cudaEventCreate(&amp;startEvent) );</span><br><span class="line">   checkCuda( cudaEventCreate(&amp;stopEvent) );</span><br><span class="line"> </span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;Offset, Bandwidth (GB/s):\n&quot;</span>);</span><br><span class="line">   </span><br><span class="line">   offset&lt;&lt;&lt;n/blockSize, blockSize&gt;&gt;&gt;(d_a, <span class="number">0</span>); <span class="comment">// warm up</span></span><br><span class="line"> </span><br><span class="line">   <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt;= <span class="number">32</span>; i++)</span><br><span class="line">   &#123;</span><br><span class="line">     checkCuda( cudaMemset(d_a, <span class="number">0</span>, n * <span class="number">33</span> * <span class="keyword">sizeof</span>(T)) );</span><br><span class="line"> </span><br><span class="line">     checkCuda( cudaEventRecord(startEvent,<span class="number">0</span>) );</span><br><span class="line">     offset&lt;&lt;&lt;<span class="built_in">ceil</span>((<span class="type">float</span>)n/blockSize), blockSize&gt;&gt;&gt;(d_a, i);</span><br><span class="line">     checkCuda( cudaEventRecord(stopEvent,<span class="number">0</span>) );</span><br><span class="line">     checkCuda( cudaEventSynchronize(stopEvent) );</span><br><span class="line"> </span><br><span class="line">     checkCuda( cudaEventElapsedTime(&amp;ms, startEvent, stopEvent) );</span><br><span class="line">     <span class="built_in">printf</span>(<span class="string">&quot;%d, %f\n&quot;</span>, i, <span class="number">2</span>*nMB/ms);</span><br><span class="line">   &#125;</span><br><span class="line"> </span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;Stride, Bandwidth (GB/s):\n&quot;</span>);</span><br><span class="line"> </span><br><span class="line">   stride&lt;&lt;&lt;n/blockSize, blockSize&gt;&gt;&gt;(d_a, <span class="number">1</span>); <span class="comment">// warm up</span></span><br><span class="line">   <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">32</span>; i++) </span><br><span class="line">   &#123;</span><br><span class="line">     checkCuda( cudaMemset(d_a, <span class="number">0</span>, n * <span class="number">33</span> * <span class="keyword">sizeof</span>(T)) );</span><br><span class="line"> </span><br><span class="line">     checkCuda( cudaEventRecord(startEvent,<span class="number">0</span>) );</span><br><span class="line">     stride&lt;&lt;&lt;n/blockSize, blockSize&gt;&gt;&gt;(d_a, i);</span><br><span class="line">     checkCuda( cudaEventRecord(stopEvent,<span class="number">0</span>) );</span><br><span class="line">     checkCuda( cudaEventSynchronize(stopEvent) );</span><br><span class="line"> </span><br><span class="line">     checkCuda( cudaEventElapsedTime(&amp;ms, startEvent, stopEvent) );</span><br><span class="line">     <span class="built_in">printf</span>(<span class="string">&quot;%d, %f\n&quot;</span>, i, <span class="number">2</span>*nMB/ms);</span><br><span class="line">   &#125;</span><br><span class="line"> </span><br><span class="line">   checkCuda( cudaEventDestroy(startEvent) );</span><br><span class="line">   checkCuda( cudaEventDestroy(stopEvent) );</span><br><span class="line">   cudaFree(d_a);</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> <span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> **argv)</span></span><br><span class="line"> &#123;</span><br><span class="line">   <span class="type">int</span> nMB = <span class="number">8</span>;</span><br><span class="line">   <span class="type">int</span> deviceId = <span class="number">0</span>;</span><br><span class="line">   <span class="type">bool</span> bFp64 = <span class="literal">false</span>;</span><br><span class="line"> </span><br><span class="line">   <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; argc; i++) &#123;    </span><br><span class="line">     <span class="keyword">if</span> (!<span class="built_in">strncmp</span>(argv[i], <span class="string">&quot;dev=&quot;</span>, <span class="number">4</span>))</span><br><span class="line">       deviceId = atoi((<span class="type">char</span>*)(&amp;argv[i][<span class="number">4</span>]));</span><br><span class="line">     <span class="keyword">else</span> <span class="keyword">if</span> (!<span class="built_in">strcmp</span>(argv[i], <span class="string">&quot;fp64&quot;</span>))</span><br><span class="line">       bFp64 = <span class="literal">true</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   </span><br><span class="line">   cudaDeviceProp prop;</span><br><span class="line">   </span><br><span class="line">   checkCuda( cudaSetDevice(deviceId));</span><br><span class="line">   checkCuda( cudaGetDeviceProperties(&amp;prop, deviceId) );</span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;Device: %s\n&quot;</span>, prop.name);</span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;Transfer size (MB): %d\n&quot;</span>, nMB);</span><br><span class="line">   </span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;%s Precision\n&quot;</span>, bFp64 ? <span class="string">&quot;Double&quot;</span> : <span class="string">&quot;Single&quot;</span>);</span><br><span class="line">   </span><br><span class="line">   <span class="keyword">if</span> (bFp64) runTest&lt;<span class="type">double</span>&gt;(deviceId, nMB);</span><br><span class="line">   <span class="keyword">else</span>       runTest&lt;<span class="type">float</span>&gt;(deviceId, nMB);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>虽然但是，这个程序哪里内存合并了T-T</p>
<h1 id="TD9-算法"><a href="#TD9-算法" class="headerlink" title="TD9: 算法"></a>TD9: 算法</h1><p>只写Kernel</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">dot</span><span class="params">( <span class="type">double</span> *a, <span class="type">double</span> *b, <span class="type">double</span> *c )</span> </span><br><span class="line">&#123;</span><br><span class="line">    __shared__ <span class="type">double</span> cache[threadsPerBlock];</span><br><span class="line">    <span class="type">int</span> tid = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="type">int</span> cacheIndex = threadIdx.x;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Pre-computation of a[i]*b[i] with segments of blockDim.x * gridDim.x values</span></span><br><span class="line">    <span class="type">double</span>   temp = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (tid &lt; N) </span><br><span class="line">    &#123;</span><br><span class="line">        temp += a[tid] * b[tid];</span><br><span class="line">        tid += blockDim.x * gridDim.x;</span><br><span class="line">    &#125;</span><br><span class="line">    cache[cacheIndex] = temp;</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    tid = threadIdx.x + blockIdx.x * blockDim.x;    </span><br><span class="line">    <span class="keyword">for</span> ( <span class="type">int</span> stride = <span class="number">1</span>; stride &lt;= blockDim.x ; stride *= <span class="number">2</span>)</span><br><span class="line">    &#123;	</span><br><span class="line">        __syncthreads () ;<span class="comment">//对吗？</span></span><br><span class="line">    	<span class="keyword">if</span> ( threadIdx.x % stride == <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">        	cache[<span class="number">2</span>*threadIdx.x] = cache[<span class="number">2</span>*threadIdx.x] + cache[<span class="number">2</span>*threadIdx.x+stride];</span><br><span class="line">    	&#125;      </span><br><span class="line">    &#125;</span><br><span class="line">    c[blockIdx.x]=cache[threadsPerBlock<span class="number">-1</span>];</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<h1 id="TD10-hist"><a href="#TD10-hist" class="headerlink" title="TD10: hist"></a>TD10: hist</h1><p>原子操作<code>atomicAdd</code>和<code>__shared__</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;text.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> NB_ASCII_CHAR 128</span></span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> threadsPerBlock = <span class="number">256</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> BlockNumber = <span class="number">16</span>;</span><br><span class="line"></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">histo_kernel</span><span class="params">( <span class="type">char</span> *buffer , <span class="type">long</span> size , <span class="type">int</span> *histo )</span></span><br><span class="line">&#123;</span><br><span class="line">	<span class="type">int</span> i = threadIdx . x + blockIdx . x * blockDim . x ;</span><br><span class="line">	<span class="comment">// stride表示总的线程数[是的，kernel这里一维]</span></span><br><span class="line">	<span class="type">int</span> stride = blockDim.x * gridDim.x ;</span><br><span class="line">	<span class="comment">// All threads handle blockDim . x * gridDim . x</span></span><br><span class="line">	<span class="comment">// consecutive elements</span></span><br><span class="line">	<span class="keyword">while</span> ( i &lt; size ) </span><br><span class="line">    &#123;</span><br><span class="line">        atomicAdd (&amp;(histo[buffer[i]]) , <span class="number">1</span>) ;</span><br><span class="line">        i += stride ;<span class="comment">//？？？，这在干嘛救命，直接去下一个bloc的这个位置？</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">histo_kernel_shared</span><span class="params">( <span class="type">char</span> *buffer , <span class="type">long</span> size , <span class="type">int</span> *histo)</span></span><br><span class="line">&#123;</span><br><span class="line">	__shared__  <span class="type">unsigned</span> <span class="type">int</span> histo_private[NB_ASCII_CHAR];</span><br><span class="line">    <span class="comment">//给shared赋值/反正都是0的意思？</span></span><br><span class="line">	<span class="keyword">if</span> ( threadIdx.x &lt; NB_ASCII_CHAR) histo_private[threadIdx.x] = <span class="number">0</span>;</span><br><span class="line">	__syncthreads () ;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> i = threadIdx.x + blockIdx.x * blockDim.x ;</span><br><span class="line">	<span class="comment">//同样的这里</span></span><br><span class="line">	<span class="type">int</span> stride = blockDim.x * gridDim.x ;</span><br><span class="line">	<span class="keyword">while</span> ( i &lt; size )</span><br><span class="line">    &#123;</span><br><span class="line">    	atomicAdd ( &amp;( histo_private[buffer[i]]) , <span class="number">1</span>) ;</span><br><span class="line">    	i += stride ;</span><br><span class="line">    &#125;</span><br><span class="line">	<span class="comment">// wait for all other threads in the block to finish</span></span><br><span class="line">	__syncthreads () ;</span><br><span class="line">	<span class="keyword">if</span> ( threadIdx.x &lt; NB_ASCII_CHAR)</span><br><span class="line">    &#123;</span><br><span class="line">		atomicAdd (&amp;( histo [ threadIdx.x ]) , histo_private[ threadIdx.x] ) ;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">( <span class="type">void</span> )</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> len = <span class="built_in">strlen</span>(h_str);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;len:%d\n&quot;</span>, len);</span><br><span class="line">    <span class="type">int</span> size = len*<span class="keyword">sizeof</span>(<span class="type">char</span>);</span><br><span class="line"></span><br><span class="line">    <span class="type">char</span> *d_str;</span><br><span class="line">    <span class="type">int</span> *h_histo, *d_histo;</span><br><span class="line"></span><br><span class="line">    cudaEvent_t start, stop;</span><br><span class="line">    cudaEventCreate( &amp;start );</span><br><span class="line">    cudaEventCreate( &amp;stop );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// GPU computation </span></span><br><span class="line">    h_histo = (<span class="type">int</span>*)<span class="built_in">malloc</span>( len*<span class="keyword">sizeof</span>(<span class="type">int</span>) );</span><br><span class="line">    cudaMalloc( (<span class="type">void</span>**)&amp;d_str, len*<span class="keyword">sizeof</span>(<span class="type">char</span>) );</span><br><span class="line">    cudaMalloc( (<span class="type">void</span>**)&amp;d_histo, len*<span class="keyword">sizeof</span>(<span class="type">int</span>) );</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    cudaEventRecord( start, <span class="number">0</span> );</span><br><span class="line">    cudaMemcpy( d_str, h_str, len*<span class="keyword">sizeof</span>(<span class="type">char</span>), cudaMemcpyHostToDevice );</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    histo_kernel_shared&lt;&lt;&lt;BlockNumber,threadsPerBlock&gt;&gt;&gt;( d_str, size, d_histo );</span><br><span class="line"></span><br><span class="line">    cudaMemcpy( h_histo, d_histo, NB_ASCII_CHAR*<span class="keyword">sizeof</span>(<span class="type">int</span>), cudaMemcpyDeviceToHost );</span><br><span class="line"> </span><br><span class="line">    cudaEventRecord( stop, <span class="number">0</span> );</span><br><span class="line">    cudaEventSynchronize( stop );</span><br><span class="line">    <span class="type">float</span> elapsedTime;</span><br><span class="line">    cudaEventElapsedTime( &amp;elapsedTime, start, stop );</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Total time for  GPU computation with shared memory was %f ms\n&quot;</span>, elapsedTime );</span><br><span class="line"></span><br><span class="line"><span class="comment">// for (int bean = 0; bean &lt; NB_ASCII_CHAR; bean++) &#123;</span></span><br><span class="line"><span class="comment">//         std::cout &lt;&lt; (char) bean &lt;&lt; &quot; : &quot; &lt;&lt; h_histo[bean] &lt;&lt; std::endl;</span></span><br><span class="line"><span class="comment">//     &#125;</span></span><br><span class="line"></span><br><span class="line">    cudaFree(d_histo);</span><br><span class="line">    cudaFree(d_str);</span><br><span class="line">    <span class="built_in">free</span>(h_histo);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// GPU computation with shared memory</span></span><br><span class="line"></span><br><span class="line">    h_histo = (<span class="type">int</span>*)<span class="built_in">malloc</span>( len*<span class="keyword">sizeof</span>(<span class="type">int</span>) );</span><br><span class="line"></span><br><span class="line">    cudaMalloc( (<span class="type">void</span>**)&amp;d_str, len*<span class="keyword">sizeof</span>(<span class="type">char</span>) );</span><br><span class="line">    cudaMalloc( (<span class="type">void</span>**)&amp;d_histo, len*<span class="keyword">sizeof</span>(<span class="type">int</span>) );</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    cudaEventRecord( start, <span class="number">0</span> );</span><br><span class="line">    cudaMemcpy( d_str, h_str, len*<span class="keyword">sizeof</span>(<span class="type">char</span>), cudaMemcpyHostToDevice );</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    histo_kernel&lt;&lt;&lt;BlockNumber,threadsPerBlock&gt;&gt;&gt;( d_str, size, d_histo );</span><br><span class="line"></span><br><span class="line">    cudaMemcpy( h_histo, d_histo, NB_ASCII_CHAR*<span class="keyword">sizeof</span>(<span class="type">int</span>), cudaMemcpyDeviceToHost );</span><br><span class="line"> </span><br><span class="line">    cudaEventRecord( stop, <span class="number">0</span> );</span><br><span class="line">    cudaEventSynchronize( stop );</span><br><span class="line">    cudaEventElapsedTime( &amp;elapsedTime, start, stop );</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Total time for naive GPU computation was %f ms\n&quot;</span>, elapsedTime );</span><br><span class="line"></span><br><span class="line"><span class="comment">// for (int bean = 0; bean &lt; NB_ASCII_CHAR; bean++) &#123;</span></span><br><span class="line"><span class="comment">//         std::cout &lt;&lt; (char) bean &lt;&lt; &quot; : &quot; &lt;&lt; h_histo[bean] &lt;&lt; std::endl;</span></span><br><span class="line"><span class="comment">//     &#125;</span></span><br><span class="line">    cudaFree(d_histo);</span><br><span class="line">    cudaFree(d_str);</span><br><span class="line">    <span class="built_in">free</span>(h_histo);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// CPU computation</span></span><br><span class="line">    </span><br><span class="line">    cudaEventRecord( start, <span class="number">0</span> );</span><br><span class="line"></span><br><span class="line">    u_int histo[NB_ASCII_CHAR] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; len; i++)&#123;</span><br><span class="line">            histo[h_str[i]]++;</span><br><span class="line">    &#125;    </span><br><span class="line">    <span class="comment">// for (int bean = 0; bean &lt; NB_ASCII_CHAR; bean++) &#123;</span></span><br><span class="line">    <span class="comment">//     std::cout &lt;&lt; (char) bean &lt;&lt; &quot; : &quot; &lt;&lt; histo[bean] &lt;&lt; std::endl;</span></span><br><span class="line">    <span class="comment">// &#125;</span></span><br><span class="line">    cudaEventRecord( stop, <span class="number">0</span> );</span><br><span class="line">    cudaEventSynchronize( stop );</span><br><span class="line">    cudaEventElapsedTime( &amp;elapsedTime, start, stop );</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Total time for CPU computation was %f ms\n&quot;</span>, elapsedTime );</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="TD11-stream"><a href="#TD11-stream" class="headerlink" title="TD11: stream"></a>TD11: stream</h1><ol>
<li>default stream指的就是我们最常见的cudaMalloc+cudaMemcpy</li>
<li>stream里面<ul>
<li>void，没有写global这些</li>
<li>申请创建stream: <code>cudaStream_t stream1; + cudaStreamCreate (&amp; stream1 ) ;</code></li>
<li>粘过去：<ul>
<li><code>cudaMemcpyAsync( d_a1 , a +i , SEGMENT_SIZE * sizeof ( int ), cudaMemcpyHostToDevice, stream1 ) ;</code></li>
<li><code>cudaMemcpyAsync( d_a2 , a +i + SEGMENT_SIZE , SEGMENT_SIZE * sizeof ( int ),cudaMemcpyHostToDevice,  stream2 ) ;</code></li>
</ul>
</li>
<li>调kernel：<code>vector_add&lt;&lt;&lt;SEGMENT_SIZE/THREADS_PER_BLOCK, THREADS_PER_BLOCK,0,stream1&gt;&gt;&gt;( d_a1, d_b1, d_c1);</code></li>
<li>粘回来：<ul>
<li><code>cudaMemcpyAsync( c + i, d_c1, SEGMENT_SIZE * sizeof ( int ) , cudaMemcpyDeviceToHost,stream1 ) ;</code></li>
<li><code>cudaMemcpyAsync(  c +i + SEGMENT_SIZE ,d_c2 , SEGMENT_SIZE * sizeof ( int ) , cudaMemcpyDeviceToHost,stream2 ) ;</code></li>
</ul>
</li>
</ul>
</li>
</ol>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;math.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> N (2048*2048)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> THREADS_PER_BLOCK 512</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> NB_STREAMS 4</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> SEGMENT_SIZE (1024*128)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CREATE_CUDAEVENT cudaEvent_t start, stop; \</span></span><br><span class="line"><span class="meta">cudaEventCreate(&amp;start); \</span></span><br><span class="line"><span class="meta">cudaEventCreate(&amp;stop);</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> START_CUDAEVENT cudaEventRecord(start, 0);</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> STOP_AND_PRINT_CUDAEVENT(txt) cudaEventRecord(stop, 0);\</span></span><br><span class="line"><span class="meta">cudaEventSynchronize(stop);\</span></span><br><span class="line"><span class="meta">&#123;float elapsedTime;\</span></span><br><span class="line"><span class="meta">cudaEventElapsedTime(&amp;elapsedTime, start, stop);\</span></span><br><span class="line"><span class="meta">printf(<span class="string">&quot;Time to %s %3.1f ms\n&quot;</span>, #txt, elapsedTime);&#125;</span></span><br><span class="line"></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">vector_add</span><span class="params">(<span class="type">int</span> *a, <span class="type">int</span> *b, <span class="type">int</span> *c)</span></span><br><span class="line">&#123;</span><br><span class="line">	<span class="type">int</span> index = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">	c[index] = a[index] + b[index];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">stream_addition</span><span class="params">(<span class="type">int</span> *a, <span class="type">int</span> *b, <span class="type">int</span> *c)</span></span><br><span class="line">&#123;</span><br><span class="line">	<span class="comment">/*&lt; Add your code here, you can use the kernel without change it &gt;*/</span></span><br><span class="line"><span class="type">int</span> size = N * <span class="keyword">sizeof</span>( <span class="type">int</span> )/NB_STREAMS;</span><br><span class="line"></span><br><span class="line">cudaStream_t stream1 , stream2 , stream3, stream4;</span><br><span class="line">cudaStreamCreate (&amp; stream1 ) ;</span><br><span class="line">cudaStreamCreate (&amp; stream2 ) ;</span><br><span class="line">cudaStreamCreate (&amp; stream3 ) ;</span><br><span class="line">cudaStreamCreate (&amp; stream4 ) ;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> *d_a1, *d_b1, *d_c1;</span><br><span class="line"><span class="type">int</span> *d_a2, *d_b2, *d_c2;</span><br><span class="line"><span class="type">int</span> *d_a3, *d_b3, *d_c3;</span><br><span class="line"><span class="type">int</span> *d_a4, *d_b4, *d_c4;</span><br><span class="line"></span><br><span class="line">cudaMalloc( &amp;d_a1, size );</span><br><span class="line">cudaMalloc( &amp;d_b1, size );</span><br><span class="line">cudaMalloc( &amp;d_c1, size );</span><br><span class="line"></span><br><span class="line">cudaMalloc( &amp;d_a2, size );</span><br><span class="line">cudaMalloc( &amp;d_b2, size );</span><br><span class="line">cudaMalloc( &amp;d_c2, size );</span><br><span class="line"></span><br><span class="line">cudaMalloc( &amp;d_a3, size );</span><br><span class="line">cudaMalloc( &amp;d_b3, size );</span><br><span class="line">cudaMalloc( &amp;d_c3, size );</span><br><span class="line"></span><br><span class="line">cudaMalloc( &amp;d_a4, size );</span><br><span class="line">cudaMalloc( &amp;d_b4, size );</span><br><span class="line">cudaMalloc( &amp;d_c4, size );</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ( <span class="type">int</span> i =<span class="number">0</span>; i &lt; N ; i += SEGMENT_SIZE *NB_STREAMS) &#123;</span><br><span class="line">cudaMemcpyAsync( d_a1 , a +i , SEGMENT_SIZE * <span class="keyword">sizeof</span> ( <span class="type">int</span> ), cudaMemcpyHostToDevice, stream1 ) ;</span><br><span class="line">cudaMemcpyAsync( d_b1 , b +i , SEGMENT_SIZE * <span class="keyword">sizeof</span> ( <span class="type">int</span> ), cudaMemcpyHostToDevice, stream1 ) ;</span><br><span class="line"></span><br><span class="line">cudaMemcpyAsync( d_a2 , a +i + SEGMENT_SIZE , SEGMENT_SIZE * <span class="keyword">sizeof</span> ( <span class="type">int</span> ),cudaMemcpyHostToDevice,  stream2 ) ;</span><br><span class="line">cudaMemcpyAsync( d_b2 , b +i + SEGMENT_SIZE, SEGMENT_SIZE * <span class="keyword">sizeof</span> ( <span class="type">int</span> ) ,cudaMemcpyHostToDevice, stream2 ) ;</span><br><span class="line"></span><br><span class="line">cudaMemcpyAsync( d_a3 , a +i + <span class="number">2</span>*SEGMENT_SIZE, SEGMENT_SIZE * <span class="keyword">sizeof</span> ( <span class="type">int</span> ) ,cudaMemcpyHostToDevice, stream3 ) ;</span><br><span class="line">cudaMemcpyAsync( d_b3 , b +i + <span class="number">2</span>* SEGMENT_SIZE, SEGMENT_SIZE * <span class="keyword">sizeof</span> ( <span class="type">int</span> ) ,cudaMemcpyHostToDevice, stream3 ) ;</span><br><span class="line"></span><br><span class="line">cudaMemcpyAsync( d_a4 , a +i + <span class="number">3</span>*SEGMENT_SIZE , SEGMENT_SIZE * <span class="keyword">sizeof</span> ( <span class="type">int</span> ) ,cudaMemcpyHostToDevice, stream4 ) ;</span><br><span class="line">cudaMemcpyAsync( d_b4 , b +i + <span class="number">3</span>*SEGMENT_SIZE , SEGMENT_SIZE * <span class="keyword">sizeof</span> ( <span class="type">int</span> ) ,cudaMemcpyHostToDevice, stream4 ) ;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vector_add&lt;&lt;&lt;SEGMENT_SIZE/THREADS_PER_BLOCK, THREADS_PER_BLOCK,<span class="number">0</span>,stream1&gt;&gt;&gt;( d_a1, d_b1, d_c1);</span><br><span class="line">vector_add&lt;&lt;&lt;SEGMENT_SIZE/THREADS_PER_BLOCK, THREADS_PER_BLOCK,<span class="number">0</span>,stream2&gt;&gt;&gt;( d_a2, d_b2, d_c2);</span><br><span class="line">vector_add&lt;&lt;&lt;SEGMENT_SIZE/THREADS_PER_BLOCK, THREADS_PER_BLOCK,<span class="number">0</span>,stream3&gt;&gt;&gt;( d_a3, d_b3, d_c3);</span><br><span class="line">vector_add&lt;&lt;&lt;SEGMENT_SIZE/THREADS_PER_BLOCK, THREADS_PER_BLOCK,<span class="number">0</span>,stream4&gt;&gt;&gt;( d_a4, d_b4, d_c4);</span><br><span class="line"></span><br><span class="line">cudaMemcpyAsync( c + i, d_c1, SEGMENT_SIZE * <span class="keyword">sizeof</span> ( <span class="type">int</span> ) , cudaMemcpyDeviceToHost,stream1 ) ;</span><br><span class="line"></span><br><span class="line">cudaMemcpyAsync(  c +i + SEGMENT_SIZE ,d_c2 , SEGMENT_SIZE * <span class="keyword">sizeof</span> ( <span class="type">int</span> ) , cudaMemcpyDeviceToHost,stream2 ) ;</span><br><span class="line"></span><br><span class="line">cudaMemcpyAsync(  c +i + <span class="number">2</span>*SEGMENT_SIZE, d_c3 , SEGMENT_SIZE * <span class="keyword">sizeof</span> ( <span class="type">int</span> ) ,cudaMemcpyDeviceToHost, stream3 ) ;</span><br><span class="line"></span><br><span class="line">cudaMemcpyAsync(  c +i + <span class="number">3</span>*SEGMENT_SIZE ,d_c4 , SEGMENT_SIZE * <span class="keyword">sizeof</span> ( <span class="type">int</span> ) ,cudaMemcpyDeviceToHost, stream4 ) ;</span><br><span class="line"></span><br><span class="line">cudaFree( d_a1 );</span><br><span class="line">cudaFree( d_b1 );</span><br><span class="line">cudaFree( d_c1 );</span><br><span class="line"></span><br><span class="line">cudaFree( d_a2 );</span><br><span class="line">cudaFree( d_b2 );</span><br><span class="line">cudaFree( d_c2 );</span><br><span class="line"></span><br><span class="line">cudaFree( d_a3 );</span><br><span class="line">cudaFree( d_b3 );</span><br><span class="line">cudaFree( d_c3 );</span><br><span class="line"></span><br><span class="line">cudaFree( d_a4 );</span><br><span class="line">cudaFree( d_b4 );</span><br><span class="line">cudaFree( d_c4 );</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">addition</span><span class="params">(<span class="type">int</span> *a, <span class="type">int</span> *b, <span class="type">int</span> *c)</span></span><br><span class="line">&#123;</span><br><span class="line">	CREATE_CUDAEVENT</span><br><span class="line">	<span class="type">int</span> size = N * <span class="keyword">sizeof</span>( <span class="type">int</span> );</span><br><span class="line">	<span class="type">int</span> *d_a, *d_b, *d_c;	</span><br><span class="line"></span><br><span class="line">	cudaMalloc( (<span class="type">void</span> **) &amp;d_a, size );</span><br><span class="line">	cudaMalloc( (<span class="type">void</span> **) &amp;d_b, size );</span><br><span class="line">	cudaMalloc( (<span class="type">void</span> **) &amp;d_c, size );</span><br><span class="line"></span><br><span class="line">	START_CUDAEVENT</span><br><span class="line">	<span class="title function_">cudaMemcpy</span><span class="params">( d_a, a, size, cudaMemcpyHostToDevice )</span>;</span><br><span class="line">	cudaMemcpy( d_b, b, size, cudaMemcpyHostToDevice );</span><br><span class="line">	STOP_AND_PRINT_CUDAEVENT(<span class="built_in">memcpy</span> h2d)</span><br><span class="line">	</span><br><span class="line">	START_CUDAEVENT</span><br><span class="line">	vector_add&lt;&lt;&lt; (N + (THREADS_PER_BLOCK<span class="number">-1</span>)) / THREADS_PER_BLOCK, THREADS_PER_BLOCK &gt;&gt;&gt;( d_a, d_b, d_c );</span><br><span class="line">	STOP_AND_PRINT_CUDAEVENT(computation)</span><br><span class="line"></span><br><span class="line">	START_CUDAEVENT</span><br><span class="line">	<span class="title function_">cudaMemcpy</span><span class="params">( c, d_c, size, cudaMemcpyDeviceToHost )</span>;</span><br><span class="line">	STOP_AND_PRINT_CUDAEVENT(<span class="built_in">memcpy</span> d2h)	</span><br><span class="line"></span><br><span class="line">	<span class="comment">/* clean up */</span></span><br><span class="line">	cudaFree( d_a );</span><br><span class="line">	cudaFree( d_b );</span><br><span class="line">	cudaFree( d_c );</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">	<span class="type">int</span> *a, *b, *c;</span><br><span class="line">	<span class="type">int</span> size = N * <span class="keyword">sizeof</span>( <span class="type">int</span> );</span><br><span class="line">	</span><br><span class="line">	<span class="comment">/* Pinned memory */</span></span><br><span class="line">	cudaHostAlloc((<span class="type">void</span> **) &amp;a, size, cudaHostAllocDefault);</span><br><span class="line">	cudaHostAlloc((<span class="type">void</span> **) &amp;b, size, cudaHostAllocDefault);</span><br><span class="line">	cudaHostAlloc((<span class="type">void</span> **) &amp;c, size, cudaHostAllocDefault);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span>( <span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++ )</span><br><span class="line">	&#123;</span><br><span class="line">		a[i] = b[i] = i;</span><br><span class="line">		c[i] = <span class="number">0</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;Addition with default stream\n&quot;</span>);</span><br><span class="line">	addition(a, b, c);</span><br><span class="line"></span><br><span class="line">	<span class="built_in">printf</span>( <span class="string">&quot;c[0] = %d\n&quot;</span>,c[<span class="number">0</span>] );</span><br><span class="line">	<span class="built_in">printf</span>( <span class="string">&quot;c[%d] = %d\n&quot;</span>,N<span class="number">-1</span>, c[N<span class="number">-1</span>] );</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span>( <span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++ )</span><br><span class="line">	&#123;	c[i] = <span class="number">0</span>; 	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">/*&lt; Add a call to your function with streams &gt;*/</span></span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;Addition with streams\n&quot;</span>);</span><br><span class="line">	stream_addition(a, b, c);</span><br><span class="line"></span><br><span class="line">	<span class="built_in">printf</span>( <span class="string">&quot;c[0] = %d\n&quot;</span>,c[<span class="number">0</span>] );</span><br><span class="line">	<span class="built_in">printf</span>( <span class="string">&quot;c[%d] = %d\n&quot;</span>, N<span class="number">-1</span>, c[N<span class="number">-1</span>] );</span><br><span class="line">	<span class="built_in">printf</span>( <span class="string">&quot;c[%d] = %d\n&quot;</span>, N/<span class="number">4</span><span class="number">-1</span>, c[N/<span class="number">4</span><span class="number">-1</span>] );</span><br><span class="line">	<span class="built_in">printf</span>( <span class="string">&quot;c[%d] = %d\n&quot;</span>, <span class="number">6</span>, c[<span class="number">6</span>] );</span><br><span class="line"></span><br><span class="line">	</span><br><span class="line">	cudaFreeHost(a);</span><br><span class="line">	cudaFreeHost(b);</span><br><span class="line">	cudaFreeHost(c);</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2023/01/31/%E5%8D%9A%E5%BC%88%E8%AE%BA1/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">博弈论(一)</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2023/01/31/GPU_CM6/"><span class="level-item">GPU_CM6</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://nililili7876.cn/2023/01/31/GPU_TD/';
            this.page.identifier = '2023/01/31/GPU_TD/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + '你来了啊' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20221029/ya微信图片_202112041747093.77tsun4ckc80.jpg" alt="7876"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">7876</p><p class="is-size-6 is-block">一个平平无奇的美女罢了</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Nantes France</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">78</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">30</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">34</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ppoffice" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#GPU-TDs"><span class="level-left"><span class="level-item">1</span><span class="level-item">GPU_TDs</span></span></a></li><li><a class="level is-mobile" href="#TD1"><span class="level-left"><span class="level-item">2</span><span class="level-item">TD1</span></span></a></li><li><a class="level is-mobile" href="#TD2-一维向量加"><span class="level-left"><span class="level-item">3</span><span class="level-item">TD2: 一维向量加</span></span></a></li><li><a class="level is-mobile" href="#TD3-kernel-multidimension"><span class="level-left"><span class="level-item">4</span><span class="level-item">TD3: kernel multidimension</span></span></a></li><li><a class="level is-mobile" href="#TD4-shared"><span class="level-left"><span class="level-item">5</span><span class="level-item">TD4: __shared__</span></span></a></li><li><a class="level is-mobile" href="#TD5-内存"><span class="level-left"><span class="level-item">6</span><span class="level-item">TD5: 内存</span></span></a></li><li><a class="level is-mobile" href="#TD6-coalesced"><span class="level-left"><span class="level-item">7</span><span class="level-item">TD6: coalesced</span></span></a></li><li><a class="level is-mobile" href="#TD9-算法"><span class="level-left"><span class="level-item">8</span><span class="level-item">TD9: 算法</span></span></a></li><li><a class="level is-mobile" href="#TD10-hist"><span class="level-left"><span class="level-item">9</span><span class="level-item">TD10: hist</span></span></a></li><li><a class="level is-mobile" href="#TD11-stream"><span class="level-left"><span class="level-item">10</span><span class="level-item">TD11: stream</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://cdn.statically.io/gh/nililili7876/blog_pic@main/20221029/ya39f1ada59d3fa9d65440760f6b18c21.jpg" alt="Bienvenue en Planète 7876" height="28"></a><p class="is-size-7"><span>&copy; 2023 7876</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>