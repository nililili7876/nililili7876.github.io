<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>GPU_CM6 - Bienvenue en Planète 7876</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#3273dc"><meta name="application-name" content="Icaurs - Hexo Theme"><meta name="msapplication-TileImage" content="icons/touch-icon-iphone.png"><meta name="msapplication-TileColor" content="#3273dc"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Icaurs - Hexo Theme"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="144x144" href="icons/touch-icon-iphone.png"><link rel="apple-touch-icon" sizes="152x152" href="icons/touch-icon-ipad.png"><link rel="apple-touch-icon" sizes="72x72" href="icon/logo.ico"><link rel="apple-touch-icon" sizes="96x96" href="icon/logo.ico"><link rel="apple-touch-icon" sizes="128x128" href="icon/logo.ico"><link rel="apple-touch-icon" sizes="256x256" href="icon/logo.ico"><meta name="description" content="CM6 原子指令：柱状图和流 PPT: 10,11"><meta property="og:type" content="blog"><meta property="og:title" content="111"><meta property="og:url" content="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20221029/ya%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20220917210647.jpg"><meta property="og:site_name" content="Bienvenue en Planète 7876"><meta property="og:description" content="CM6 原子指令：柱状图和流 PPT: 10,11"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230116/yaimage.79f7eza29lo0.png"><meta property="og:image" content="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230116/yaimage.56t0cbfvtuc0.png"><meta property="og:image" content="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230116/yaimage.3hpf726iw2y0.png"><meta property="og:image" content="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230116/yaimage.ia1un4nyea8.png"><meta property="og:image" content="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230116/yaimage.4cuttxw2fjy0.png"><meta property="og:image" content="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230116/yaimage.45e7vmwftxi0.png"><meta property="og:image" content="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230116/yaimage.3nv9exaoff00.png"><meta property="og:image" content="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230117/yaimage.3baiv4y21de0.png"><meta property="og:image" content="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230117/yaimage.58g6pjdcvuc0.png"><meta property="og:image" content="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230117/yaimage.iuc263yr5fk.png"><meta property="og:image" content="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230117/yaimage.7avsadgiyvk0.png"><meta property="og:image" content="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230117/yaimage.163h4uww44w0.png"><meta property="og:image" content="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230117/yaimage.70attstyb9s0.png"><meta property="article:published_time" content="2023-01-30T20:19:24.664Z"><meta property="article:modified_time" content="2023-01-30T20:50:01.121Z"><meta property="article:author" content="7876"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230116/yaimage.79f7eza29lo0.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20221029/ya微信图片_20220917210647.jpg"},"headline":"GPU_CM6","image":["https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230116/yaimage.79f7eza29lo0.png","https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230116/yaimage.56t0cbfvtuc0.png","https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230116/yaimage.3hpf726iw2y0.png","https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230116/yaimage.ia1un4nyea8.png","https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230116/yaimage.4cuttxw2fjy0.png","https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230116/yaimage.45e7vmwftxi0.png","https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230116/yaimage.3nv9exaoff00.png","https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230117/yaimage.3baiv4y21de0.png","https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230117/yaimage.58g6pjdcvuc0.png","https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230117/yaimage.iuc263yr5fk.png","https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230117/yaimage.7avsadgiyvk0.png","https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230117/yaimage.163h4uww44w0.png","https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230117/yaimage.70attstyb9s0.png"],"datePublished":"2023-01-30T20:19:24.664Z","dateModified":"2023-01-30T20:50:01.121Z","author":{"@type":"Person","name":7876},"publisher":{"@type":"Organization","name":"Bienvenue en Planète 7876","logo":{"@type":"ImageObject","url":"https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20221029/ya39f1ada59d3fa9d65440760f6b18c21.jpg"}},"description":"CM6 原子指令：柱状图和流 PPT: 10,11"}</script><link rel="canonical" href="http://nililili7876.cn/2023/01/30/GPU_CM6/"><link rel="icon" href="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20221029/ya39f1ada59d3fa9d65440760f6b18c21.jpg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20221029/ya39f1ada59d3fa9d65440760f6b18c21.jpg" alt="Bienvenue en Planète 7876" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com">GitHub</a><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-01-30T20:19:24.664Z" title="30/01/2023 21:19:24">2023-01-30</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-01-30T20:50:01.121Z" title="30/01/2023 21:50:01">2023-01-30</time></span><span class="level-item"><a class="link-muted" href="/categories/NANTES/">NANTES</a><span> / </span><a class="link-muted" href="/categories/NANTES/EI2/">EI2</a><span> / </span><a class="link-muted" href="/categories/NANTES/EI2/EI2-2/">EI2_2</a><span> / </span><a class="link-muted" href="/categories/NANTES/EI2/EI2-2/GPU/">GPU</a></span><span class="level-item">15 minutes read (About 2226 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">GPU_CM6</h1><div class="content"><h1 id="cm6"><a class="markdownIt-Anchor" href="#cm6"></a> CM6</h1>
<p>原子指令：柱状图和流</p>
<p>PPT: 10,11</p>
<span id="more"></span>
<h1 id="1-柱状图"><a class="markdownIt-Anchor" href="#1-柱状图"></a> 1. 柱状图</h1>
<p>PPT上是这样来定义一个柱状图的：数据集中的每个元素都与一个“容器”[conteneur]相关联,比如我们把每4个字母设成一个conteneur,那么对于&quot;When nine hundred years old you reach, look as good you will not&quot;这句话，有：</p>
<p><img src="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230116/yaimage.79f7eza29lo0.png" alt="yaimage" /></p>
<p>并行算法设计</p>
<ol>
<li>
<p>把输入分成几个sections</p>
</li>
<li>
<p>每个线程对应一个section作为输入</p>
</li>
<li>
<p>每个线程在它的部分中迭代</p>
</li>
<li>
<p>对于每个字母，容器计数器都会增加</p>
</li>
</ol>
<p><img src="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230116/yaimage.56t0cbfvtuc0.png" alt="yaimage" /></p>
<p>对内存访问进行优化</p>
<p>上面的方法的分段操作导致内存访问效率低</p>
<ul>
<li>线程不访问相邻的数据</li>
<li>访问未分组(合并)[coalesced]</li>
<li>DRAM带宽未得到充分利用</li>
</ul>
<p>所以我们要对进入的section进行交叉</p>
<p><img src="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230116/yaimage.3hpf726iw2y0.png" alt="yaimage" /></p>
<p>Accès concurrent</p>
<p>数据的更新分为3个步骤</p>
<ol>
<li>old &lt;- Mem[x]</li>
<li>new &lt;- old + 1</li>
<li>Mem[x] &lt;- new</li>
</ol>
<p>由于线程的并发访问，可能会出现不一致</p>
<center class="half">
    <img src="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230116/yaimage.ia1un4nyea8.png" width="400"height="200"/><img src="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230116/yaimage.4cuttxw2fjy0.png" width="400"height="200"/>
</center>
<p>原子操作的Principe</p>
<ol>
<li>读-修改-写操作在单个硬件指令中执行</li>
<li>硬件确保在当前原子操作完成之前，其他线程不能在同一位置执行另一个读-修改-写操作
<ul>
<li>任何试图在同一位置执行原子操作的其他线程都将被放置在队列中</li>
<li>所有线程对同一个内存地址串联执行原子操作</li>
</ul>
</li>
<li>通过调用转换为单个指令的函数来执行: 加、减、递增、递减、最小、最大、交换、CAS(比较和交换)</li>
<li>对应的CUDA实例：<code>int atomicAdd(int* adresse, int val); </code>
<ul>
<li>读：从全局内存或共享内存中地址指向的位置读取旧的32位字，</li>
<li>修改：计算(old + val)</li>
<li>写：将结果存储在相同地址的内存中,返回旧值</li>
<li>这三个操作在一条指令中执行</li>
</ul>
</li>
</ol>
<p>算hist的cuda代码1.0</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">__global__</span><br><span class="line"><span class="type">void</span> <span class="title function_">histo_kernel</span> <span class="params">( <span class="type">unsigned</span> <span class="type">char</span> * buffer , <span class="type">long</span> size , <span class="type">unsigned</span> <span class="type">int</span> * histo )</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> i = threadIdx .x + blockIdx .x * blockDim .x;</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// stride表示总的线程数,感觉主要就是这步</span></span><br><span class="line">	<span class="type">int</span> stride = blockDim.x * gridDim.x;</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// All threads handle blockDim .x * gridDim .x</span></span><br><span class="line">	<span class="comment">// consecutive elements</span></span><br><span class="line">	<span class="keyword">while</span> (i &lt; size )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//实际算字符串啥的这里就不一定是buffer[i]了--&gt;buffer [i] - ’a’</span></span><br><span class="line">		atomicAdd ( &amp;( histo [ buffer [i]]) , <span class="number">1</span>) ;</span><br><span class="line">		i += stride ;<span class="comment">//跳到下一个线程不应该一个blockDim就行了吗？为什么要加所有的线程数哦</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>原子操作从DRAM读取开始，它有几百个周期的延迟</li>
<li>原子操作以在同一个地方写入结束，延迟了几百个周期</li>
<li>在此期间，其他人无法访问该地址</li>
<li>每次读-修改-写都会导致两次内存访问延迟</li>
<li>对单个变量(DRAM位置)的所有原子操作都是序列化的</li>
</ol>
<p><img src="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230116/yaimage.45e7vmwftxi0.png" alt="yaimage" /></p>
<p>代码2.0[privatisation]</p>
<p><img src="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230116/yaimage.3nv9exaoff00.png" alt="yaimage" /></p>
<p>???</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">histo_kernel</span> <span class="params">( <span class="type">unsigned</span> <span class="type">char</span> * buffer , <span class="type">long</span> size , <span class="type">unsigned</span> <span class="type">int</span> * histo )</span></span><br><span class="line">&#123;</span><br><span class="line">    __shared__ <span class="type">unsigned</span> <span class="type">int</span> histo_private[<span class="number">7</span>];</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> ( threadIdx .x &lt; <span class="number">7</span>) histo_private [ threadidx .x] = <span class="number">0</span>;</span><br><span class="line">	__syncthreads () ;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> i = threadIdx .x + blockIdx .x * blockDim .x;</span><br><span class="line">	<span class="comment">// stride is total number of threads</span></span><br><span class="line">	<span class="type">int</span> stride = blockDim .x * gridDim .x;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (i &lt; size ) </span><br><span class="line">    &#123;</span><br><span class="line">		atomicAdd ( &amp;( private_histo [ buffer [i ]/<span class="number">4</span>) , <span class="number">1</span>) ;</span><br><span class="line">		i += stride ;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">// wait for all other threads in the block to finish</span></span><br><span class="line">	__syncthreads () ;</span><br><span class="line">                                      </span><br><span class="line">                                      </span><br><span class="line">    <span class="keyword">if</span> ( threadIdx .x &lt; <span class="number">7</span>) </span><br><span class="line">    &#123;</span><br><span class="line">		atomicAdd (&amp;(histo[threadIdx.x]) , private_histo [ threadIdx .x] );</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="2-stream"><a class="markdownIt-Anchor" href="#2-stream"></a> 2. stream</h1>
<h2 id="21-序列化数据传输和计算"><a class="markdownIt-Anchor" href="#21-序列化数据传输和计算"></a> 2.1 序列化数据传输和计算</h2>
<p>以VecAddKernel()为例说明的cudaMemcpy序列化数据传输和GPU计算的示意图</p>
<p><img src="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230117/yaimage.3baiv4y21de0.png" alt="yaimage" /></p>
<p>一些CUDA设备允许同时执行kernel，同时在设备[périphérique]和主机[mémoire hôte]内存之间复制数据.具体做法</p>
<ul>
<li>Diviser les grands vecteurs en segments</li>
<li>叠加传输和计算相邻的段[ segments]</li>
</ul>
<p><img src="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230117/yaimage.58g6pjdcvuc0.png" alt="yaimage" /></p>
<p>通过biais du pipeling的concurrence</p>
<ul>
<li>管道: 一种连接多个程序的方式，使得一个程序的输出可以作为另一个程序的输入</li>
</ul>
<p><img src="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230117/yaimage.iuc263yr5fk.png" alt="yaimage" /></p>
<h2 id="22-cuda-stream"><a class="markdownIt-Anchor" href="#22-cuda-stream"></a> 2.2 CUDA stream</h2>
<h3 id="221-定义"><a class="markdownIt-Anchor" href="#221-定义"></a> 2.2.1 定义</h3>
<ol>
<li>
<p>Synchrone vs asynchrone:</p>
<ul>
<li>Synchrone：排队作业并等待其完成</li>
<li>Asynchrone：排队作业并立即返回</li>
<li>kernel的启动是异步[asynchrone]的，自动与主机重叠[overlap]</li>
</ul>
</li>
<li>
<p>stream</p>
<p>CUDA prend en charge[支持] l’exécution parallèle des kernels et de cudaMemcpy() avec des streams :</p>
<ul>
<li>每个流是设备上的一个操作队列（内核启动和cudaMemcpy()调用）
<ul>
<li>host将该作业放在队列中，并立即继续</li>
<li>当资源空闲时，device会从流中安排工作</li>
</ul>
</li>
<li>来自不同数据流的操作（任务）可以并行执行（任务并行）</li>
</ul>
</li>
</ol>
<h3 id="222-stream的执行顺序"><a class="markdownIt-Anchor" href="#222-stream的执行顺序"></a> 2.2.2 stream的执行顺序</h3>
<ol>
<li>
<p>一个steam</p>
<p>host上的代码发出的请求被放在队列[FIFO]中</p>
<ul>
<li>队列是由驱动器和设备异步读取和处理的</li>
<li>驱动程序确保队列中的操作被按顺序处理</li>
<li>示意图：<img src="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230117/yaimage.7avsadgiyvk0.png" alt="yaimage" /></li>
</ul>
</li>
<li>
<p>多个streams</p>
<p>CUDA允许host线程查询并与单个队列(即stream)同步，示意图</p>
<p><img src="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230117/yaimage.163h4uww44w0.png" alt="yaimage" /></p>
</li>
</ol>
<h3 id="223-代码"><a class="markdownIt-Anchor" href="#223-代码"></a> 2.2.3 代码</h3>
<p>先申明stream变量 : <code>cudaStream_t stream</code></p>
<ul>
<li>
<p>申请stream : <code>cudaStreamCreate (&amp; stream )</code></p>
</li>
<li>
<p>毁掉stream: <code>cudaStreamDestroy ( stream )</code></p>
</li>
</ul>
<p>stream是kernel函数的第4个参数：<code>kernel &lt;&lt;&lt; blocks , threads , smem , stream &gt;&gt;&gt;() </code>, smem: shared memory,用于指定每个 CUDA 核的共享内存大小</p>
<p>stream的传递[异步拷贝:在调用它时会立即返回，而数据拷贝操作可能在后台进行]：<code>cudaMemcpyAsync ( dst , src , size , dir , stream )</code>, dir: cudaMemcpyHostToDevice/cudaMemcpyDeviceToHost/cudaMemcpyDeviceToDevice</p>
<p>默认：除非另有说明，所有调用都放在默认流中，通常称为“stream 0”, 他有特殊的同步规则: 与所有流同步:stream 0中的操作不能<em>重叠</em>其他流，比如说</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cudaStream_t stream1 ;</span><br><span class="line">cudaStreamCreate (&amp; stream1 ) ;</span><br><span class="line">foo &lt;&lt;&lt; blocks , threads &gt;&gt;&gt;() ; <span class="comment">// Stream par defaut</span></span><br><span class="line">foo &lt;&lt;&lt; blocks , threads ,<span class="number">0</span> , stream1 &gt;&gt;&gt;() ; <span class="comment">// pas de concurrence</span></span><br><span class="line">cudaStreamDestroy ( stream1 );</span><br></pre></td></tr></table></figure>
<p>特例：stream avec drapeau non-bloquant à l’allocation <code>cudaStreamCreateWithFlags (&amp; stream , cudaStreamNonBlocking )</code>,这种处理需要</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cudaStream_t stream1 ;</span><br><span class="line">cudaStreamCreateWithFlags (&amp; stream1 , cudaStreamNonBlocking );</span><br><span class="line">foo &lt;&lt;&lt; blocks , threads &gt;&gt;&gt;() ; <span class="comment">// Stream par defaut</span></span><br><span class="line">foo &lt;&lt;&lt; blocks , threads ,<span class="number">0</span> , stream1 &gt;&gt;&gt;() ; <span class="comment">// Concurrence,天哎这不是没区别</span></span><br><span class="line">cudaStreamDestroy ( stream1 );</span><br></pre></td></tr></table></figure>
<p>一个host使用Multi-Stream的简单的例子1.0</p>
<p>说明：在下列情况下获得并发concurrence:</p>
<ol>
<li>
<p>传输是在非默认流中进行的</p>
</li>
<li>
<p>副本是异步的</p>
</li>
<li>
<p>每个方向一次只能转移一次</p>
</li>
<li>
<p>host的内存是pinned的</p>
</li>
</ol>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cudaStream_t stream0 , stream1 ;</span><br><span class="line">cudaStreamCreate (&amp;stream0 );</span><br><span class="line">cudaStreamCreate (&amp;stream1 );</span><br><span class="line"><span class="type">float</span> *d_A0 , *d_B0 , * d_C0 ; <span class="comment">// device memory for stream 0</span></span><br><span class="line"><span class="type">float</span> *d_A1 , *d_B1 , * d_C1 ; <span class="comment">// device memory for stream 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// cudaMalloc () calls for d_A0 , d_B0 , d_C0 , d_A1 , d_B1 , d_C1 go here</span></span><br><span class="line"><span class="comment">//...</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i =<span class="number">0</span>; i&lt;n; i+= SegSize *<span class="number">2</span>) </span><br><span class="line">&#123;</span><br><span class="line">	cudaMemcpyAsync(d_A0,h_A+i,SegSize*<span class="keyword">sizeof</span>(<span class="type">float</span>),cudaMemcpyHostToDevice, stream0 );</span><br><span class="line">	cudaMemcpyAsync(d_B0,h_B+i,SegSize*<span class="keyword">sizeof</span>(<span class="type">float</span>),cudaMemcpyHostToDevice, stream0 );</span><br><span class="line">	vecAdd&lt;&lt;&lt;SegSize /<span class="number">256</span> , <span class="number">256</span> , <span class="number">0</span> , stream0&gt;&gt;&gt;(d_A0 , d_B0 ,d_C0) ;</span><br><span class="line">	cudaMemcpyAsync(h_C+i,d_C0,SegSize*<span class="keyword">sizeof</span>(<span class="type">float</span>) ,cudaMemcpyDeviceToHost, stream0 );</span><br><span class="line"></span><br><span class="line">    cudaMemcpyAsync(d_A1,h_A+i,SegSize*<span class="keyword">sizeof</span>(<span class="type">float</span>),cudaMemcpyHostToDevice, stream1 );</span><br><span class="line">	cudaMemcpyAsync(d_B1,h_B+i,SegSize*<span class="keyword">sizeof</span>(<span class="type">float</span>),cudaMemcpyHostToDevice, stream1 );</span><br><span class="line">	vecAdd&lt;&lt;&lt;SegSize /<span class="number">256</span> , <span class="number">256</span> , <span class="number">0</span> , stream0&gt;&gt;&gt;(d_A1 , d_B1 ,d_C1) ;<span class="comment">//这里不太确定是不是写d_C1</span></span><br><span class="line">	cudaMemcpyAsync(d_C1,h_C+i+SegSize,SegSize*<span class="keyword">sizeof</span>(<span class="type">float</span>) ,cudaMemcpyDeviceToHost, stream1);<span class="comment">//注意这行哈</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>示意图</p>
<p><img src="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20230117/yaimage.70attstyb9s0.png" alt="yaimage" /></p>
<p>上面的向量加2.0</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">cudaStream_t stream0 , stream1 ;</span><br><span class="line">cudaStreamCreate (&amp;stream0 );</span><br><span class="line">cudaStreamCreate (&amp;stream1 );</span><br><span class="line"><span class="type">float</span> *d_A0 , *d_B0 , * d_C0 ; <span class="comment">// device memory for stream 0</span></span><br><span class="line"><span class="type">float</span> *d_A1 , *d_B1 , * d_C1 ; <span class="comment">// device memory for stream 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// cudaMalloc () calls for d_A0 , d_B0 , d_C0 , d_A1 , d_B1 , d_C1 go here</span></span><br><span class="line"><span class="comment">//...</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i =<span class="number">0</span>; i&lt;n; i+= SegSize *<span class="number">2</span>) </span><br><span class="line">&#123;</span><br><span class="line">	cudaMemcpyAsync(d_A0,h_A+i,SegSize*<span class="keyword">sizeof</span>(<span class="type">float</span>),cudaMemcpyHostToDevice, stream0 );</span><br><span class="line">	cudaMemcpyAsync(d_B0,h_B+i,SegSize*<span class="keyword">sizeof</span>(<span class="type">float</span>),cudaMemcpyHostToDevice, stream0 );</span><br><span class="line">	</span><br><span class="line">	cudaMemcpyAsync(d_A1,h_A+i,SegSize*<span class="keyword">sizeof</span>(<span class="type">float</span>),cudaMemcpyHostToDevice, stream1 );</span><br><span class="line">	cudaMemcpyAsync(d_B1,h_B+i,SegSize*<span class="keyword">sizeof</span>(<span class="type">float</span>),cudaMemcpyHostToDevice, stream1 );</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line">	vecAdd&lt;&lt;&lt;SegSize /<span class="number">256</span> , <span class="number">256</span> , <span class="number">0</span> , stream0&gt;&gt;&gt;(d_A0 , d_B0 ,d_C0) ;</span><br><span class="line">	vecAdd&lt;&lt;&lt;SegSize /<span class="number">256</span> , <span class="number">256</span> , <span class="number">0</span> , stream0&gt;&gt;&gt;(d_A1 , d_B1 ,d_C1) ;<span class="comment">//这里不太确定是不是写d_C1</span></span><br><span class="line"></span><br><span class="line">	cudaMemcpyAsync(h_C+i,d_C0,SegSize*<span class="keyword">sizeof</span>(<span class="type">float</span>) ,cudaMemcpyDeviceToHost, stream0 );</span><br><span class="line">    cudaMemcpyAsync(d_C1,h_C+i+SegSize,SegSize*<span class="keyword">sizeof</span>(<span class="type">float</span>) ,cudaMemcpyDeviceToHost, stream1);<span class="comment">//注意这行哈</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="23-同步和cuda-event"><a class="markdownIt-Anchor" href="#23-同步和cuda-event"></a> 2.3 同步和CUDA event</h2>
<p>全部同步：将host给bloque直到所有的CUDA调用完成：<code>cudaDeviceSynchronize ()</code></p>
<p>将主机与特定的流同步:阻塞主机，直到流中发出的所有CUDA调用完成：<code>cudaStreamSynchronize ( stream )</code></p>
<p>使用event同步host或device</p>
<ul>
<li>create/destroy：<code>cudaEventCreate(&amp;event)</code>/ <code> cudaEventDestroy(&amp;event)</code></li>
<li>禁用同步以提高性能并避免同步问题:<code>cudaEventCreateWithFlags(&amp;event, cudaEventDisableTiming)</code></li>
<li><code>cudaEventRecord(&amp;event, stream)</code>
<ul>
<li>将event状态设置为未发生</li>
<li>把event放到流的等待队列里面</li>
<li>event状态被定义为当它到达流的开始时发生的</li>
</ul>
</li>
<li>如果发生事件，则返回CUDA_SUCCESS: <code>cudaEventQuery ( event )</code></li>
<li>阻塞host，直到流完成所有正在进行的调用: <code>cudaEventSynchronize ( event )</code></li>
<li><code>cudaStreamWaitEvent ( stream, event )</code>
<ul>
<li>阻塞流，直到事件发生</li>
<li>只在调用后阻止启动,不阻塞host</li>
</ul>
</li>
</ul>
<p>注意：需要申明 cudaEventSynchronize <strong>avant</strong> cudaEventRecord</p>
<h2 id="24-préfetching-de-données数据预取"><a class="markdownIt-Anchor" href="#24-préfetching-de-données数据预取"></a> 2.4 Préfetching de données/数据预取</h2>
<p>CUDA统一内存中的数据预取[Préfetching de données ]</p>
<ul>
<li>éviter les défauts de page</li>
<li>提供一种机制，通过建立数据定位来提高应用性能</li>
<li>用于将数据迁移到设备或主机，并在处理器开始使用数据之前将页表映射到处理器</li>
<li>当从单个处理器访问数据时最有用</li>
<li><code>__host__ cudaError_t cudaMemPrefetchAsync ( const void* devPtr, size_t count, int dstDevice, cudaStream_t stream = 0 )</code></li>
</ul>
<p>实例</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// data must’ve been allocated with cudaMallocManaged ( ( void **) &amp;data , N);</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">function</span> <span class="params">( <span class="type">int</span> * data , cudaStream_t stream )</span> </span><br><span class="line">&#123;</span><br><span class="line">    init (data , N) ;</span><br><span class="line">    cudaMemPrefetchAsync (data , N * <span class="keyword">sizeof</span> ( <span class="type">int</span>) , myGpuId , stream );</span><br><span class="line">    kernel &lt; &lt; &lt;... , stream &gt; &gt; &gt;(data , ...) ;</span><br><span class="line">    <span class="comment">// Prefetch to the host cudaStreamSynchronize ( stream );</span></span><br><span class="line">    cudaMemPrefetchAsync (data , N * <span class="keyword">sizeof</span> ( <span class="type">int</span>) , cudaCpuDeviceId , stream );</span><br><span class="line">    hostFunction (data , N) ;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2023/01/30/GPU_TD/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">GPU_TDs</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2023/01/30/GPU_CM5/"><span class="level-item">GPU_CM5</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://nililili7876.cn/2023/01/30/GPU_CM6/';
            this.page.identifier = '2023/01/30/GPU_CM6/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + '你来了啊' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20221029/ya微信图片_202112041747093.77tsun4ckc80.jpg" alt="7876"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">7876</p><p class="is-size-6 is-block">一个平平无奇的美女罢了</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Nantes France</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">69</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">28</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">32</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ppoffice" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#cm6"><span class="level-left"><span class="level-item">1</span><span class="level-item"> CM6</span></span></a></li><li><a class="level is-mobile" href="#1-柱状图"><span class="level-left"><span class="level-item">2</span><span class="level-item"> 1. 柱状图</span></span></a></li><li><a class="level is-mobile" href="#2-stream"><span class="level-left"><span class="level-item">3</span><span class="level-item"> 2. stream</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#21-序列化数据传输和计算"><span class="level-left"><span class="level-item">3.1</span><span class="level-item"> 2.1 序列化数据传输和计算</span></span></a></li><li><a class="level is-mobile" href="#22-cuda-stream"><span class="level-left"><span class="level-item">3.2</span><span class="level-item"> 2.2 CUDA stream</span></span></a></li><li><a class="level is-mobile" href="#23-同步和cuda-event"><span class="level-left"><span class="level-item">3.3</span><span class="level-item"> 2.3 同步和CUDA event</span></span></a></li><li><a class="level is-mobile" href="#24-préfetching-de-données数据预取"><span class="level-left"><span class="level-item">3.4</span><span class="level-item"> 2.4 Préfetching de données/数据预取</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://cdn.staticaly.com/gh/nililili7876/blog_pic@main/20221029/ya39f1ada59d3fa9d65440760f6b18c21.jpg" alt="Bienvenue en Planète 7876" height="28"></a><p class="is-size-7"><span>&copy; 2023 7876</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>